"""
ğŸ§ âœ¨ README â€” End-to-End EMG â†” Kinematics & Proximal â†” Distal Pipeline (drop-in doc cell) âœ¨ğŸ§ 
Put this cell at the very bottom of your notebook/script for a quick, visually engaging overview of what the code above does.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸš€ Quick Start
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Dependencies: numpy, pandas, matplotlib, scipy, statsmodels, scikit-learn, spm1d, joblib, openpyxl/xlsxwriter
â€¢ Raw file expected: sEMG data HSP/Timeseries completo.csv
Must have columns like: ID, SOTTOGRUPPO, TRIAL, Percentage, PHASE, COGNOME, NOME, RF, VM, VL, GM, GL, and knee kinematics (Angle/Vel/Acc).
â€¢ Groups are harmonized: NORMATIVIâ†’NESSUNO, GRUPPO1â†’1, GRUPPO2â†’2, GRUPPO3â†’3.
â€¢ All figures/tables land in outputs/.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#1 PRE-PROCESSING â€” envelopes, normalization, per-group caches
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Goal: Turn raw EMG into clean, comparable envelopes; build group_signals for later sections; export subject-level features.

Key functions (WHAT â†’ WHY):
â€¢ band_pass_filter(data, lowcut, highcut, fs, order=4)
â†’ Butterworth 20â€“400 Hz (typical EMG band) to remove motion/DC drift and keep physiology.
â€¢ rico(alpha, order, cutoff, dt) + cascaded filtfilt
â†’ Gentle extra low-pass stages to stabilize the rectified signal before envelope.
â€¢ coa_evaluation(signal)
â†’ â€œCenter of Activityâ€ timing metric (0â€“1) when parameter=CoA.
â€¢ Main loop: per subject/trial â†’ band-pass â†’ cascade LP â†’ rectify â†’ envelope (10 Hz) â†’ normalize by per-subject max â†’ resample to 0â€“100% â†’ stash in group_signals[group][muscle][phase].
â€¢ Feature write-out: Parameters4.xlsx with Max/Mean/FWHM/CoA per muscle.

Why this matters: normalization removes scale differences; 0â€“100 resampling makes whole-cycle stats/plots consistent; group_signals feeds everything else.

Outputs:
â€¢ group_signals memory cache â€¢ Parameters4.xlsx â€¢ overview plots per muscleÃ—group and overlays.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#2 WHOLE-CYCLE SPM (nonparametric) â€” Holm/FDR + clean plots
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Goal: Compare full time-series between groups with cluster-wise inference, no hand-picked windows.

Key functions (WHAT â†’ WHY):
â€¢ _stack_wholecycle(group_signals, groups, muscle)
â†’ Build (n_obs Ã— n_time) arrays plus group labels for SPM.
â€¢ _spm_anova_nonparam(y, A) / _spm_pairwise_nonparam(G_arrays)
â†’ Nonparam ANOVA and pairwise t with permutations; robust to non-Gaussian residuals.
â€¢ _cluster_rows_from_spmi(spmi, time_percent, stat_kind)
â†’ Extract cluster start/end/center/width, cluster-p, peak stat; falls back to threshold rebuild if needed.
â€¢ _adjust_pvalues(pvals, method="holm"/"fdr_bh")
â†’ Family-wise control across all clusters (FWER & FDR).
â€¢ _save_spm_tables(omnibus_df, pairwise_df, path)
â†’ Excel first, CSV fallback â€” reproducible reporting.
â€¢ plot_spm_fdr_with_q(...)
â†’ SPM curves with shaded spans and exact FDR q labels per cluster.

Why this matters: SPM keeps temporal contiguity; Holm (conservative) + FDR (powerful) provide transparent multiple-testing control.

Outputs:
â€¢ spm_clusters_corrected.xlsx (omnibus + pairwise with Holm/FDR) â€¢ Optional SPM figures with q-labels.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#3 EMG â†” KINEMATICS (by phase) â€” XCorr, HAC-OLS, Granger + tv-VAR IRFs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Goal: Quantify bidirectional coupling of EMG (RF/VM/VL) and knee kinematics (Angle/Vel/Acc) within each phase.

Data handling (WHAT â†’ WHY):
â€¢ ensure_subject_from_names(...) / assign_trials(...)
â†’ Stable subject keys & trial wraps from names + monotone %gait.
â€¢ merge_phasewise_robust(EMG, KIN, tolerance=0.5)
â†’ Nearest-time asof-merge per phase (subject-aware) to handle tiny timing mismatches.

Descriptive & inference:
â€¢ _pearson_xcorr(y, x, max_lag)
â†’ z-scored XCorr; lag>0 â‡’ y lags x (lead/lag reading).
â€¢ HAC OLS helpers (hac_maxlags, compute_pmax, xcorr_maxlag)
â†’ p-values and RÂ² resilient to autocorrelation/heteroskedasticity.
â€¢ Granger (min p across lags) + per-PhaseÃ—Group BH-FDR
â†’ Directional evidence with multiplicity control.

Time-varying dynamics (tv-VAR + IRFs):
â€¢ build_spline_basis / tvvar_design / fit_tvvar_ridge
â†’ Ridge-regularized time-varying VAR along gait (%).
â€¢ A_of_tau / companion_from_Alist
â†’ Evaluate VAR at any % gait and form companion dynamics.
â€¢ tvvar_irf(..., orth="generalized"|"chol") & tvvar_fevd_shares
â†’ Phase-aware impulse responses + FEVD.
â€¢ Horizon selection: _energy_truncate, _first_zero_horizon, _fevd_plateau_horizon, choose_optimal_horizon
â†’ Pick a sensible IRF length per phase (energy + FEVD plateau default).
â€¢ _select_tvvar_for_pair(...) + _irf_summary_stats(...) + add_irf_metrics(...)
â†’ Gentle order selection, and rich IRF features (peaks, areas, energy, zero-crossings, durations).

Why this matters: within-phase, bidirectional modeling respects biomechanics; HAC prevents false positives; tv-VAR captures slow coefficient drift over gait.

Outputs:
â€¢ results_irf.xlsx â€¢ 3Ã—3 grids (z-scored traces + driver bands), â€œcontroller flowmapsâ€, and EMG traces with directional coloring.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#4 PROXIMAL (RF/VM/VL) â†” DISTAL (GM/GL) â€” EMGâ†”EMG directionality
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Goal: Test if proximal muscles are more informative than distal ones per phase; summarize with a Directionality Index (DI).

Key functions (WHAT â†’ WHY):
â€¢ normalize_phase_names(df, col="Phase")
â†’ Force canonical 7-phase labels/order; unknowns are flagged/dropped in visuals.
â€¢ ensure_distal_emg(avg_emg, df_raw, require_cols=("GM","GL"))
â†’ Rebuild GM/GL from raw via per-bin RMS if absent; otherwise set NaN (analysis still runs).
â€¢ _analyze_pair(phase_df, time_col, prox, dist)
â†’ For each Proxâ€“Dist pair: XCorr (both dirs), HAC-OLS p & RÂ² (raw), standardized Î² (effect size), fast VAR(p=2) Granger p.
â€¢ analyze_prox_vs_dist(avg_emg)
â†’ Parallel over all pairs and GroupÃ—Phase; adds FDR per GroupÃ—Phase (q_Pâ†’D, q_Dâ†’P).
â€¢ summarize_by_phase(pd_df, alpha, use_fdr=True)
â†’ Aggregates to PhaseÃ—Group: n_pairs, n_sig_Pâ†’D, n_sig_Dâ†’P, DI, mean|Î²*| and median|XCorr|.
â€¢ run_emg2emg_prox_vs_dist(...)
â†’ One-shot runner returning pd_df (pairwise) and phase_summary.
â€¢ visualize_emg2emg(pd_df, phase_summary, out_dir)
â†’ DI heatmap + per-phase bar charts (DI, counts, mean|Î²*|).
â€¢ visualize_prox_vs_dist_colormapped(group_signals, pd_df, phase_summary, group, out_dir)
â†’ EMG time-series shaded: blue = Proxâ†’Dist, red = Distâ†’Prox, intensity âˆ |Î²*|; GM/GL overlays for context.

Directionality Index (per PhaseÃ—Group):
DI = ( #sig(Proxâ†’Dist) âˆ’ #sig(Distâ†’Prox) ) / total_pairs â€ƒ(using Granger + FDR @ q<.05).
+1 = entirely proximal-driven; âˆ’1 = entirely distal-driven.

Outputs:
â€¢ emg_prox_vs_dist_results.xlsx (+ CSVs) â€¢ emg2emg_DI_heatmap.png â€¢ per-phase bar charts â€¢ color-mapped EMG timelines.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“¦ What shows up in outputs/
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Parameters4.xlsx â€” preprocessing features
â€¢ spm_clusters_corrected.xlsx â€” SPM clusters (Holm/FDR)
â€¢ results_irf.xlsx â€” EMGâ†”KIN (XCorr/HAC/Granger/FDR + IRFs)
â€¢ optionA_grid_<group>.png, controller_flowmap_<group>.png, EMG-KIN colored grids
â€¢ Proxâ†”Dist suite: emg2emg_DI_heatmap.png, per-phase bar charts, colormapped traces
â€¢ Pairwise & summary CSV/XLSX for reproducibility

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§ª Metric cheatsheet
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ XCorr lag: >0 â‡’ response lags predictor (predictor leads).
â€¢ HAC-OLS: p & RÂ² with heteroskedasticity/autocorrelation-robust SEs.
â€¢ Granger: min p across lags; BH-FDR within GroupÃ—Phase.
â€¢ IRF: time-varying impulse response; stats include peaks, areas, energy, zero-crossings.
â€¢ DI (Proxâ€“Dist): summarizes per-phase balance of Proxâ†’Dist vs Distâ†’Prox evidence.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’¡ Practical tips
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ If GM/GL are missing, Section 4 tries to derive them from raw; otherwise theyâ€™re NaN and pairs drop gracefully.
â€¢ Increase MIN_POINTS for stricter filtering; decrease for exploratory runs.
â€¢ If your phase labels differ, adjust the phase-mapping dicts at the top of each section.
â€¢ For speed: keep FAST_MODE=True, reduce SPM NP_ITER, and let Section-4 use joblib parallelism.""""""

# ğŸ§  EMG â†” Kinematics & Proximal â†” Distal (EMGâ†”EMG) â€” End-to-End Pipeline

> Clean envelopes âœ whole-cycle nonparametric SPM âœ tv-VAR IRFs (EMGâ†”KIN) âœ Proximal vs Distal EMG directionality  
> All phases respected. All statistics exported. All figures saved to `outputs/`.

---

## ğŸ”— Table of Contents

- [Quick Start](#-quick-start)
- [What You Get](#-what-you-get)
- [Data & Conventions](#-data--conventions)
- [SECTION 1 â€” Pre-processing](#section-1--preprocessing)
- [SECTION 2 â€” Whole-cycle SPM (nonparametric) + Holm/FDR](#section-2--whole-cycle-spm-nonparametric--holmfdr)
- [SECTION 3 â€” EMGâ€“Kinematics (tv-VAR + IRFs + XCorr/HAC/Granger)](#section-3--emgkinematics-tv-var--irfs--xcorhhacgranger)
- [SECTION 4 â€” Proximal (RF/VM/VL) vs Distal (GM/GL) EMGâ†”EMG](#section-4--proximal-rfvmvl-vs-distal-gmgl-emgemg)
- [Outputs (files & figures)](#-outputs-files--figures)
- [Tuning Knobs](#-tuning-knobs)
- [Troubleshooting](#-troubleshooting)
- [Glossary](#-glossary)

---

## ğŸš€ Quick Start

```bash
# 1) Install
pip install numpy pandas matplotlib scipy statsmodels scikit-learn spm1d joblib openpyxl xlsxwriter

# 2) Place your raw CSV (Italian field names supported)
#    sEMG data HSP/Timeseries completo.csv

# 3) Run the script (Jupyter or CLI)
python your_script.py


##================================================================##
# SECTION 1 == PREPROCESSING ==##
##==============================================================##

# --- Imports ---
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import butter, filtfilt
from scipy.interpolate import interp1d

# --- Custom Functions ---
def band_pass_filter(data, lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    b, a = butter(order, [lowcut / nyq, highcut / nyq], btype='band')
    return filtfilt(b, a, data)

def rico(alpha, order, cutoff, dt):
    nyq = 0.5 / dt
    b, a = butter(order, cutoff / nyq)
    return b, a

def coa_evaluation(signal):
    t = np.linspace(0, 1, len(signal))
    numerator = np.sum(t * signal)
    denominator = np.sum(signal)
    return numerator / denominator if denominator != 0 else 0

# --- Configuration ---
excel_name = 'Parameters4.xlsx'
t = 1  # Choose: 1=Max, 2=Mean, 3=FWHM, 4=CoA
n_phases = [8, 8, 8, 1]
parameter = ['Max', 'Mean', 'FWHM', 'CoA']
group_mapping = {'NESSUNO': 'NORMATIVI', 1.0: 'GRUPPO1', 2.0: 'GRUPPO2', 3.0: 'GRUPPO3'}
groups = list(group_mapping.values())
muscles = ['RF', 'VM', 'VL', 'GL', 'GM']
complete_df = pd.DataFrame()

# --- Load CSV with proper parsing ---
df = pd.read_csv('sEMG data HSP/Timeseries completo.csv', delimiter=';', decimal=',', low_memory=False)
print("âœ… CSV loaded. Shape:", df.shape)

for col in muscles:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# --- Initialize storage ---
group_signals = {
    group: {muscle: [[] for _ in range(n_phases[t-1])] for muscle in muscles}
    for group in groups
}

# --- Main Loop ---
for raw_group, group in group_mapping.items():
    if raw_group == 'NESSUNO':
        group_df = df[df['SOTTOGRUPPO'].astype(str).str.upper().str.strip() == 'NESSUNO'].copy()
    else:
        group_df = df[pd.to_numeric(df['SOTTOGRUPPO'], errors='coerce') == raw_group].copy()

    print(f"\nğŸ” Processing group: {group} (Raw: {raw_group}), Rows found: {len(group_df)}")
    if group_df.empty:
        print(f"âš ï¸ Skipping group {group} because it's empty.")
        continue

    subjects = group_df['ID'].unique()
    fs = 1000.0
    fNy = fs / 2
    all_subject_results = []

    for subject in subjects:
        subj_data = group_df[group_df['ID'] == subject]
        trials = subj_data['TRIAL'].unique()
        env_signals = {m: [] for m in muscles}
        Max = np.zeros((len(trials), len(muscles)))

        for idx, trial in enumerate(trials[:3]):
            trial_data = subj_data[subj_data['TRIAL'] == trial]
            for m_idx, muscle in enumerate(muscles):
                raw_signal = trial_data[muscle].dropna().values
                if len(raw_signal) < 10:
                    continue
                try:
                    filt_signal = band_pass_filter(raw_signal, 20, 400, fs)
                    for arm in range(1, 9):
                        b, a = rico(0.01, 2, 50 * arm, 1 / fs)
                        filt_signal = filtfilt(b, a, filt_signal)
                    rect_signal = np.abs(filt_signal)
                    b_env, a_env = butter(4, 10 / fNy)
                    env = filtfilt(b_env, a_env, rect_signal)
                    Max[idx, m_idx] = np.max(env)
                    env_signals[muscle].append(env)
                except Exception as e:
                    print(f"âš ï¸ Error processing {muscle}, trial {trial}: {e}")

        normalized = {m: [] for m in muscles}
        for m_idx, muscle in enumerate(muscles):
            max_val = np.max(Max[:, m_idx])
            if max_val == 0 or np.isnan(max_val):
                continue
            for idx in range(min(3, len(env_signals[muscle]))):
                normalized[muscle].append(env_signals[muscle][idx] / max_val)

        result = {'ID': subject, 'PATHOLOGY': group}
        for m_idx, muscle in enumerate(muscles):
            values = []
            for phase_idx in range(n_phases[t-1]):
                combined_signals = []
                for idx in range(len(normalized[muscle])):
                    signal = normalized[muscle][idx]
                    if len(signal) == 0:
                        continue
                    interp = interp1d(np.arange(len(signal)), signal, kind='linear', fill_value="extrapolate")
                    resampled = interp(np.linspace(0, len(signal) - 1, 100))
                    combined_signals.append(resampled)
                    group_signals[group][muscle][phase_idx].append(resampled)

                if not combined_signals:
                    continue

                combined_signals = np.array(combined_signals)
                if parameter[t-1] == 'Max':
                    values.append(np.max(combined_signals))
                elif parameter[t-1] == 'Mean':
                    values.append(np.mean(combined_signals))
                elif parameter[t-1] == 'FWHM':
                    max_sig = np.max(combined_signals)
                    min_sig = np.min(combined_signals)
                    threshold = (max_sig - min_sig) / 2
                    fwhm = np.sum(combined_signals > threshold, axis=1).mean()
                    values.append(fwhm)
                elif parameter[t-1] == 'CoA':
                    coa_vals = [coa_evaluation(sig) for sig in combined_signals]
                    values.append(np.mean(coa_vals))

            if values:
                result[muscle] = np.mean(values)
        all_subject_results.append(result)

    df_group = pd.DataFrame(all_subject_results)
    print(f"âœ… Finished group {group}. Subjects processed: {len(df_group)}")
    complete_df = pd.concat([complete_df, df_group], ignore_index=True)

# --- Export to Excel ---
print("ğŸ“¦ Final complete_df shape before export:", complete_df.shape)
complete_df.to_excel(excel_name, index=False, sheet_name=parameter[t-1])
print(f"âœ… Excel file '{excel_name}' written.")

# --- Plotting ---
colors = {
    'NORMATIVI': 'black',
    'GRUPPO1': 'red',
    'GRUPPO2': 'blue',
    'GRUPPO3': 'green'
}

# --- Subplot grid: Muscle x Group ---
plt.figure(figsize=(16, 9))
for m_idx, muscle in enumerate(muscles):
    for g_idx, group in enumerate(groups):
        signals = np.array(group_signals[group][muscle][0])
        if signals.size == 0 or np.all(np.isnan(signals)):
            continue

        mean_signal = np.mean(signals, axis=0)
        std_signal = np.std(signals, axis=0)
        x = np.linspace(0, 100, len(mean_signal))

        subplot_index = m_idx * len(groups) + g_idx + 1
        plt.subplot(5, 4, subplot_index)
        plt.plot(x, mean_signal, color=colors[group])
        plt.fill_between(x, mean_signal - std_signal, mean_signal + std_signal, color=colors[group], alpha=0.2)
        plt.ylim([0, 1])
        plt.title(f"{muscle} - {group}")
        plt.xlabel("Gait Cycle (%)")

plt.tight_layout()
plt.show()

# --- Overlay: All groups per muscle ---
for muscle in muscles:
    plt.figure(figsize=(10, 6))
    for group in groups:
        signals = np.array(group_signals[group][muscle][0])
        if signals.size == 0 or np.all(np.isnan(signals)):
            continue

        mean_signal = np.mean(signals, axis=0)
        std_signal = np.std(signals, axis=0)
        x = np.linspace(0, 100, len(mean_signal))

        plt.plot(x, mean_signal, label=group, color=colors[group])
        plt.fill_between(x, mean_signal - std_signal, mean_signal + std_signal, color=colors[group], alpha=0.2)

    plt.title(f"{muscle} - Normalized EMG (All Groups Overlaid)")
    plt.xlabel("Gait Cycle (%)")
    plt.ylabel("Amplitude (Normalized)")
    plt.ylim([0, 1])
    plt.grid(False)
    plt.legend()
    plt.tight_layout()
    plt.show()    

##==========================================================================================##
# ===================== SECTION 2 WHOLE-CYCLE SPM (Nonparam) + Holm/FDR + PLOTS =====================
##==========================================================================================##

import numpy as np
import pandas as pd
import itertools
import matplotlib.pyplot as plt
import spm1d

# ---------------- Config ---------------- #
ALPHA = 0.05                # target alpha (family-wise, over time)
NP_ITER = 2000              # permutations for nonparametric SPM
MIN_CURVES_PER_GROUP = 5    # require at least this many curves per group
MUSCLES = ('RF','VM','VL', '0GM', 'GL')  # muscles to analyze
SAVE_PATH = "spm_clusters_corrected.xlsx"  # set None to skip file save
MAKE_PLOTS = True
# ---------------------------------------------------------------------------------------- #

# ---------- P-value corrections (Holm & FDR-BH) ----------
def _adjust_pvalues(pvals, method="holm"):
    p = np.array([v for v in pvals], dtype=float)
    m = p.size
    if m == 0:
        return np.array([])
    order = np.argsort(p)
    rev = np.empty_like(order); rev[order] = np.arange(m)

    if method.lower() == "holm":
        adj = (m - np.arange(m)) * p[order]
        for i in range(1, m):
            adj[i] = max(adj[i], adj[i-1])
        adj = np.minimum(adj, 1.0)
        return adj[rev]

    if method.lower() in ("fdr_bh","bh","fdr"):
        adj = np.empty(m, float)
        prev = 1.0
        for i in range(m-1, -1, -1):
            val = p[order][i] * m / (i+1)
            prev = min(prev, val)
            adj[i] = prev
        adj = np.minimum(adj, 1.0)
        return adj[rev]

    return p  # no change

# ---------- Robust saver ----------
def _save_spm_tables(omnibus_df, pairwise_df, path=SAVE_PATH):
    if not path:
        return
    base = path.rsplit(".", 1)[0]
    try:
        with pd.ExcelWriter(path, engine="xlsxwriter") as w:
            omnibus_df.to_excel(w, index=False, sheet_name="Omnibus_F")
            pairwise_df.to_excel(w, index=False, sheet_name="Pairwise_t")
        print(f"Saved Excel: {path} (xlsxwriter)")
        return
    except ModuleNotFoundError:
        pass
    try:
        with pd.ExcelWriter(path, engine="openpyxl") as w:
            omnibus_df.to_excel(w, index=False, sheet_name="Omnibus_F")
            pairwise_df.to_excel(w, index=False, sheet_name="Pairwise_t")
        print(f"Saved Excel: {path} (openpyxl)")
        return
    except Exception as e:
        print(f"Excel engines unavailable ({e}). Saving CSV fallbacksâ€¦")
    omnibus_df.to_csv(f"{base}_Omnibus_F.csv", index=False)
    pairwise_df.to_csv(f"{base}_Pairwise_t.csv", index=False)
    print(f"Saved: {base}_Omnibus_F.csv and {base}_Pairwise_t.csv")

# ---------- Build whole-cycle arrays (one set per muscle) ----------
def _stack_wholecycle(group_signals, groups, muscle):
    """
    Returns:
        y : (n_obs, n_time) stacked curves
        A : (n_obs,) group labels
        G_arrays : dict[group] -> (n_g, n_time)

    Uses the first non-empty phase per group/muscle (since your data currently duplicates across phases).
    """
    G_arrays, n_time = {}, None
    for g in groups:
        phases = group_signals.get(g, {}).get(muscle, [])
        arrs_raw = None
        for ph in phases:
            if ph and len(ph) > 0:
                arrs_raw = ph
                break
        if not arrs_raw:
            G_arrays[g] = None
            continue
        arrs = [np.asarray(a, float) for a in arrs_raw if a is not None and len(a)]
        arrs = [a for a in arrs if np.all(np.isfinite(a))]
        if not arrs:
            G_arrays[g] = None
            continue
        Ls = [len(a) for a in arrs]
        L = int(np.median(Ls)) if len(set(Ls)) > 1 else Ls[0]
        if any(len(a) != L for a in arrs):
            arrs = [np.interp(np.linspace(0, len(a)-1, L), np.arange(len(a)), a) for a in arrs]
        G_arrays[g] = np.vstack(arrs)
        n_time = L if n_time is None else min(n_time, L)

    y_list, A_list, keep = [], [], {}
    for g in groups:
        A = G_arrays.get(g)
        if A is None:
            continue
        A = A[:, :n_time]
        if A.shape[0] >= MIN_CURVES_PER_GROUP:
            y_list.append(A)
            A_list.extend([g]*A.shape[0])
            keep[g] = A
    if not y_list:
        return None, None, {}
    y = np.vstack(y_list)
    A = np.array(A_list, dtype=object)
    return y, A, keep

# ---------- Nonparametric SPM wrappers ----------
def _spm_anova_nonparam(y, A):
    A = np.asarray(A); _, A_int = np.unique(A, return_inverse=True)
    A_int = A_int.astype(int)
    spm = spm1d.stats.nonparam.anova1(y, A_int)
    return spm.inference(alpha=ALPHA, iterations=NP_ITER)

def _spm_pairwise_nonparam(G_arrays):
    out = []
    gps = list(G_arrays.keys())
    pairs = list(itertools.combinations(gps, 2))
    alpha_pair = ALPHA / max(1, len(pairs))  # Bonferroni within muscle
    for gA, gB in pairs:
        yA, yB = G_arrays[gA], G_arrays[gB]
        test = spm1d.stats.nonparam.ttest2(yA, yB)
        spmi = test.inference(alpha=alpha_pair, two_tailed=True, iterations=NP_ITER)
        out.append(((gA, gB), spmi, alpha_pair))
    return out

# ---------- Robust cluster extraction (handles param & nonparam; rebuilds if needed) ----------
def _clusters_to_table(spmi, time_percent):
    """
    Returns: DataFrame columns:
      cluster, start_%, end_%, center_%, width_%, cluster_p, peak_stat, sig
    """
    rows = []
    z = np.asarray(getattr(spmi, "z", []), float).ravel()
    alpha_used = float(getattr(spmi, "alpha", ALPHA))
    zstar = float(getattr(spmi, "zstar", np.nan))

    def _idx_from_cluster(c):
        for attr in ("J", "I", "inds", "indices", "index"):
            if hasattr(c, attr):
                try:
                    idx = np.asarray(getattr(c, attr)).astype(int).ravel()
                    if idx.size:
                        return idx
                except Exception:
                    pass
        jmin = getattr(c, "jmin", None); jmax = getattr(c, "jmax", None)
        if jmin is not None and jmax is not None:
            return np.arange(int(jmin), int(jmax) + 1)
        return None

    def _p_from_cluster(c):
        for attr in ("P", "p", "Pvalue", "p_value", "pval"):
            if hasattr(c, attr):
                try:
                    val = float(getattr(c, attr))
                    if np.isfinite(val):
                        return val
                except Exception:
                    pass
        return np.nan

    # 1) Preferred: official clusters (includes per-cluster p when available)
    used_official = False
    clusters = getattr(spmi, "clusters", []) or []
    if clusters and z.size:
        for i, c in enumerate(clusters, start=1):
            idx = _idx_from_cluster(c)
            if idx is None or idx.size == 0:
                continue
            i0, i1 = int(np.min(idx)), int(np.max(idx))
            start_p = float(time_percent[i0])
            end_p   = float(time_percent[i1])
            center  = 0.5 * (start_p + end_p)
            width   = end_p - start_p
            pval    = _p_from_cluster(c)
            peak    = float(np.nanmax(np.abs(z[idx]))) if z.size else np.nan
            sig     = (np.isfinite(pval) and (pval < alpha_used))
            rows.append({
                "cluster": i,
                "start_%": start_p,
                "end_%": end_p,
                "center_%": center,
                "width_%": width,
                "cluster_p": pval,
                "peak_stat": peak,
                "sig": sig
            })
        used_official = len(rows) > 0

    # 2) Fallback: reconstruct from threshold (matches what the plot shades)
    if (not used_official) and z.size and np.isfinite(zstar):
        two_sided = np.any(z < 0)
        mask = (np.abs(z) > zstar) if two_sided else (z > zstar)
        idx_all = np.where(mask)[0]
        if idx_all.size:
            splits = np.where(np.diff(idx_all) > 1)[0] + 1
            blocks = np.split(idx_all, splits)
            for i, b in enumerate(blocks, start=1):
                if b.size == 0:
                    continue
                i0, i1 = int(np.min(b)), int(np.max(b))
                start_p = float(time_percent[i0])
                end_p   = float(time_percent[i1])
                center  = 0.5 * (start_p + end_p)
                width   = end_p - start_p
                peak    = float(np.nanmax(np.abs(z[b])))
                rows.append({
                    "cluster": i,
                    "start_%": start_p,
                    "end_%": end_p,
                    "center_%": center,
                    "width_%": width,
                    "cluster_p": np.nan,   # p not available in threshold rebuild
                    "peak_stat": peak,
                    "sig": True            # exceeded corrected threshold
                })

    return pd.DataFrame(rows)

# ---------- Plots ----------
def _plot_group_means(G_arrays, title):
    plt.figure(figsize=(8,4))
    x = np.linspace(0, 100, next(iter(G_arrays.values())).shape[1])
    for g, M in G_arrays.items():
        m, s = M.mean(axis=0), M.std(axis=0)
        plt.plot(x, m, label=g); plt.fill_between(x, m-s, m+s, alpha=0.2)
    plt.ylim(0, 1); plt.xlabel("Gait Cycle (%)"); plt.ylabel("Amplitude (normalized)")
    plt.title(title); plt.legend(); plt.tight_layout(); plt.show()

def _plot_spm(spmi, title):
    plt.figure(figsize=(8,4))
    spmi.plot()
    try:
        spmi.plot_threshold_label(); spmi.plot_p_values()
    except Exception:
        pass
    plt.title(title); plt.tight_layout(); plt.show()

# ---------- Main runner: nonparam SPM + tables + plots ----------
def run_spm_wholecycle_nonparam(group_signals, groups, muscles=MUSCLES,
                                save_path=SAVE_PATH, make_plots=MAKE_PLOTS):
    omni_rows, pair_rows = [], []
    for muscle in muscles:
        y, A, G_arrays = _stack_wholecycle(group_signals, groups, muscle)
        if y is None or len(np.unique(A)) < 2:
            print(f"Skipping {muscle}: insufficient data.")
            continue

        T = y.shape[1]
        time_percent = np.linspace(0, 100, T)

        # Descriptive plot
        if make_plots:
            _plot_group_means(G_arrays, f"{muscle} | Group means Â± SD")

        # Omnibus (FORCED NONPARAM)
        spmi_F = _spm_anova_nonparam(y, A)
        if make_plots:
            _plot_spm(spmi_F, f"{muscle} | SPM{{F}} (nonparam, Î±={ALPHA})")
        F_tbl = _clusters_to_table(spmi_F, time_percent)
        if F_tbl.empty:
            omni_rows.append({"MUSCLE": muscle, "TEST": "ANOVA1 F",
                              "cluster": np.nan, "start_%": np.nan, "end_%": np.nan,
                              "center_%": np.nan, "width_%": np.nan,
                              "cluster_p": np.nan, "peak_stat": np.nan, "sig": False})
        else:
            for _, r in F_tbl.iterrows():
                omni_rows.append({"MUSCLE": muscle, "TEST": "ANOVA1 F", **r.to_dict()})

        # Pairwise (FORCED NONPARAM; Bonferroni within muscle)
        for (gA, gB), spmi_t, alpha_pair in _spm_pairwise_nonparam(G_arrays):
            if make_plots:
                _plot_spm(spmi_t, f"{muscle} | {gA} vs {gB} | SPM{{t}} (nonparam, Î±={alpha_pair:.3g})")
            t_tbl = _clusters_to_table(spmi_t, time_percent)
            if t_tbl.empty:
                pair_rows.append({"MUSCLE": muscle, "TEST": f"{gA} vs {gB} (t)",
                                  "cluster": np.nan, "start_%": np.nan, "end_%": np.nan,
                                  "center_%": np.nan, "width_%": np.nan,
                                  "cluster_p": np.nan, "peak_stat": np.nan, "sig": False})
            else:
                for _, r in t_tbl.iterrows():
                    pair_rows.append({"MUSCLE": muscle, "TEST": f"{gA} vs {gB} (t)", **r.to_dict()})

    omnibus_df = pd.DataFrame(omni_rows)
    pairwise_df = pd.DataFrame(pair_rows)

    if save_path:
        _save_spm_tables(omnibus_df, pairwise_df, save_path)

    return omnibus_df, pairwise_df

# ---------- Apply Holm & FDR across families (omnibus vs pairwise separately) ----------
def apply_familywise_corrections(omnibus_df, pairwise_df, alpha=ALPHA):
    out_omni = omnibus_df.copy()
    out_pair = pairwise_df.copy()

    # Omnibus: adjust across ALL omnibus clusters (all muscles combined)
    mask_o = out_omni["cluster_p"].notna()
    p_o = out_omni.loc[mask_o, "cluster_p"].to_numpy(float)
    if p_o.size:
        out_omni.loc[mask_o, "p_holm"] = _adjust_pvalues(p_o, method="holm")
        out_omni.loc[mask_o, "q_fdr"]  = _adjust_pvalues(p_o, method="fdr_bh")
        out_omni["sig_holm"] = out_omni["p_holm"] <= alpha
        out_omni["sig_fdr"]  = out_omni["q_fdr"]  <= alpha

    # Pairwise: adjust across ALL pairwise clusters (all muscles & pairs combined)
    mask_p = out_pair["cluster_p"].notna()
    p_p = out_pair.loc[mask_p, "cluster_p"].to_numpy(float)
    if p_p.size:
        out_pair.loc[mask_p, "p_holm"] = _adjust_pvalues(p_p, method="holm")
        out_pair.loc[mask_p, "q_fdr"]  = _adjust_pvalues(p_p, method="fdr_bh")
        out_pair["sig_holm"] = out_pair["p_holm"] <= alpha
        out_pair["sig_fdr"]  = out_pair["q_fdr"]  <= alpha

    return out_omni, out_pair

# ------------------------ RUN (nonparam) + corrections + PLOTS ------------------------
omni_df_raw, pair_df_raw = run_spm_wholecycle_nonparam(
    group_signals=group_signals,
    groups=groups,
    muscles=MUSCLES,
    save_path=None,          # skip saving raw here
    make_plots=MAKE_PLOTS    # True to show figures
)

omni_df, pair_df = apply_familywise_corrections(omni_df_raw, pair_df_raw, alpha=ALPHA)

# Save corrected tables (xlsxwriter -> openpyxl -> CSV fallback)
_save_spm_tables(omni_df, pair_df, path=SAVE_PATH)

# ------------------------ DISPLAY Excel-like previews ------------------------
try:
    from IPython.display import display
except Exception:
    display = None

fmt = {"start_%":"{:.1f}", "end_%":"{:.1f}", "center_%":"{:.1f}", "width_%":"{:.1f}",
       "cluster_p":"{:.4f}", "peak_stat":"{:.2f}", "p_holm":"{:.4f}", "q_fdr":"{:.4f}"}

print("\n=== Omnibus_F (corrected) â€” first 20 rows ===")
if display: display(omni_df.head(20).style.format(fmt))
else: print(omni_df.head(20).to_string(index=False))

print("\n=== Pairwise_t (corrected) â€” first 20 rows ===")
if display: display(pair_df.head(20).style.format(fmt))
else: print(pair_df.head(20).to_string(index=False))

print(f"\nShapes -> Omnibus: {omni_df.shape}, Pairwise: {pair_df.shape}")
# =========================================================================================

##---Second set of plots--##
# ============================================================
# SPM PLOTS WITH FDR-ONLY ANNOTATIONS 
# ============================================================
# FIX: always capture cluster p-values + FDR q, then plot q's
# ============================================================
from scipy.stats import t as tdist, f as fdist

def _cluster_rows_from_spmi(spmi, time_percent, stat_kind='t'):
    """
    Extract clusters with p-values when spm1d exposes them; otherwise
    rebuild spans and keep p=NaN (a later step can approximate p from peak).
    Returns a DataFrame with: cluster, start_%, end_%, center_%, width_%,
    cluster_p, peak_stat, df1, df2, stat_kind
    """
    rows = []
    z = np.asarray(getattr(spmi, "z", []), float).ravel()
    zstar = float(getattr(spmi, "zstar", np.nan))
    # Try to get dfs (shape differs for F vs t)
    df1 = df2 = np.nan
    if hasattr(spmi, "df"):
        try:
            d = getattr(spmi, "df")
            if isinstance(d, (tuple, list, np.ndarray)) and len(d) >= 2:
                df1, df2 = float(d[0]), float(d[1])
            else:
                # t test stores a single df
                df1 = float(d); df2 = np.nan
        except Exception:
            pass

    # helper to read cluster p
    def _p_from_cluster_obj(c):
        for attr in ("P","p","Pvalue","p_value","pval","prob"):
            if hasattr(c, attr):
                try:
                    pv = float(getattr(c, attr))
                    if np.isfinite(pv):
                        return pv
                except Exception:
                    pass
        return np.nan

    # helper to read indices
    def _idx_from_cluster_obj(c):
        for attr in ("J","I","inds","indices","index","ind"):
            if hasattr(c, attr):
                try:
                    idx = np.asarray(getattr(c, attr)).astype(int).ravel()
                    if idx.size: return idx
                except Exception:
                    pass
        # last resort: jmin/jmax
        jmin = getattr(c, "jmin", None); jmax = getattr(c, "jmax", None)
        if jmin is not None and jmax is not None:
            return np.arange(int(jmin), int(jmax)+1)
        return None

    used_official = False
    clusters = getattr(spmi, "clusters", None)
    if clusters is None:
        clusters = getattr(spmi, "_clusters", None)
    if clusters:
        for i, c in enumerate(clusters, start=1):
            idx = _idx_from_cluster_obj(c)
            if idx is None or idx.size == 0: 
                continue
            i0, i1 = int(np.min(idx)), int(np.max(idx))
            start_p = float(time_percent[i0]); end_p = float(time_percent[i1])
            center  = 0.5*(start_p + end_p); width = end_p - start_p
            peak    = float(np.nanmax(np.abs(z[idx]))) if z.size else np.nan
            cp      = _p_from_cluster_obj(c)
            rows.append({
                "cluster": i, "start_%": start_p, "end_%": end_p,
                "center_%": center, "width_%": width,
                "cluster_p": cp, "peak_stat": peak,
                "df1": df1, "df2": df2, "stat_kind": stat_kind
            })
        used_official = len(rows) > 0

    # Fallback: rebuild from threshold if official doesn't carry
    if (not used_official) and z.size and np.isfinite(zstar):
        two_sided = np.any(z < 0)
        mask = (np.abs(z) > zstar) if two_sided else (z > zstar)
        idx_all = np.where(mask)[0]
        if idx_all.size:
            splits = np.where(np.diff(idx_all) > 1)[0] + 1
            blocks = np.split(idx_all, splits)
            for i, b in enumerate(blocks, start=1):
                if b.size == 0: continue
                i0, i1 = int(np.min(b)), int(np.max(b))
                start_p = float(time_percent[i0]); end_p = float(time_percent[i1])
                center  = 0.5*(start_p + end_p); width = end_p - start_p
                peak    = float(np.nanmax(np.abs(z[b])))
                rows.append({
                    "cluster": i, "start_%": start_p, "end_%": end_p,
                    "center_%": center, "width_%": width,
                    "cluster_p": np.nan,            # unknown here
                    "peak_stat": peak,
                    "df1": df1, "df2": df2, "stat_kind": stat_kind
                })
    return pd.DataFrame(rows)

# Swap your old _clusters_to_table with the new one in the pipelines:
def run_spm_wholecycle_nonparam(group_signals, groups, muscles=MUSCLES,
                                save_path=SAVE_PATH, make_plots=MAKE_PLOTS):
    omni_rows, pair_rows = [], []
    for muscle in muscles:
        y, A, G_arrays = _stack_wholecycle(group_signals, groups, muscle)
        if y is None or len(np.unique(A)) < 2:
            print(f"Skipping {muscle}: insufficient data.")
            continue
        T = y.shape[1]
        time_percent = np.linspace(0, 100, T)

        if make_plots:
            _plot_group_means(G_arrays, f"{muscle} | Group means Â± SD")

        # Omnibus
        spmi_F = _spm_anova_nonparam(y, A)
        if make_plots:
            _plot_spm(spmi_F, f"{muscle} | SPM{{F}} (nonparam, Î±={ALPHA})")
        F_tbl = _cluster_rows_from_spmi(spmi_F, time_percent, stat_kind='F')
        if F_tbl.empty:
            omni_rows.append({"MUSCLE": muscle, "TEST": "ANOVA1 F"})
        else:
            for _, r in F_tbl.iterrows():
                omni_rows.append({"MUSCLE": muscle, "TEST": "ANOVA1 F", **r.to_dict()})

        # Pairwise
        for (gA, gB), spmi_t, alpha_pair in _spm_pairwise_nonparam(G_arrays):
            if make_plots:
                _plot_spm(spmi_t, f"{muscle} | {gA} vs {gB} | SPM{{t}} (nonparam, Î±={alpha_pair:.3g})")
            t_tbl = _cluster_rows_from_spmi(spmi_t, time_percent, stat_kind='t')
            if t_tbl.empty:
                pair_rows.append({"MUSCLE": muscle, "TEST": f"{gA} vs {gB} (t)"})
            else:
                for _, r in t_tbl.iterrows():
                    pair_rows.append({"MUSCLE": muscle, "TEST": f"{gA} vs {gB} (t)", **r.to_dict()})
    return pd.DataFrame(omni_rows), pd.DataFrame(pair_rows)

def _approx_p_from_peak(row):
    """Fallback p when cluster_p is NaN: use peak_stat and dfs."""
    stat = float(row.get("peak_stat", np.nan))
    if not np.isfinite(stat): return np.nan
    kind = str(row.get("stat_kind", "t")).lower()
    df1  = row.get("df1", np.nan); df2 = row.get("df2", np.nan)
    try:
        if kind == 't':
            if not np.isfinite(df1): return np.nan
            return 2.0*tdist.sf(abs(stat), df=df1)
        else:  # 'F'
            if not (np.isfinite(df1) and np.isfinite(df2)): return np.nan
            return fdist.sf(stat, dfn=df1, dfd=df2)
    except Exception:
        return np.nan

def apply_familywise_corrections(omnibus_df, pairwise_df, alpha=ALPHA):
    out_omni = omnibus_df.copy()
    out_pair = pairwise_df.copy()

    # ---------- Omnibus ----------
    if not out_omni.empty:
        # fill p when missing
        if "cluster_p" in out_omni.columns:
            mask_nan = out_omni["cluster_p"].isna()
            if mask_nan.any():
                out_omni.loc[mask_nan, "cluster_p"] = out_omni.loc[mask_nan].apply(_approx_p_from_peak, axis=1)
        else:
            out_omni["cluster_p"] = out_omni.apply(_approx_p_from_peak, axis=1)

        mask = out_omni["cluster_p"].notna()
        if mask.any():
            pvals = out_omni.loc[mask, "cluster_p"].to_numpy(float)
            out_omni.loc[mask, "p_holm"] = _adjust_pvalues(pvals, "holm")
            out_omni.loc[mask, "q_fdr"]  = _adjust_pvalues(pvals, "fdr_bh")
            out_omni["sig_holm"] = out_omni["p_holm"] <= alpha
            out_omni["sig_fdr"]  = out_omni["q_fdr"]  <= alpha

    # ---------- Pairwise ----------
    if not out_pair.empty:
        if "cluster_p" in out_pair.columns:
            mask_nan = out_pair["cluster_p"].isna()
            if mask_nan.any():
                out_pair.loc[mask_nan, "cluster_p"] = out_pair.loc[mask_nan].apply(_approx_p_from_peak, axis=1)
        else:
            out_pair["cluster_p"] = out_pair.apply(_approx_p_from_peak, axis=1)

        mask = out_pair["cluster_p"].notna()
        if mask.any():
            pvals = out_pair.loc[mask, "cluster_p"].to_numpy(float)
            out_pair.loc[mask, "p_holm"] = _adjust_pvalues(pvals, "holm")
            out_pair.loc[mask, "q_fdr"]  = _adjust_pvalues(pvals, "fdr_bh")
            out_pair["sig_holm"] = out_pair["p_holm"] <= alpha
            out_pair["sig_fdr"]  = out_pair["q_fdr"]  <= alpha

    return out_omni, out_pair

# ---------- Plotter that annotates exact q beside each span ----------
plt.rcParams["figure.dpi"] = 120
FIGSIZE = (9, 3.6)
FDR_ALPHA = 0.05
Q_DECIMALS = 3

def _fmt_q(q):
    return f"q = {q:.{Q_DECIMALS}f}" if np.isfinite(q) else ""

def _xticks_percent(ax, T):
    ax.set_xlim(0, T-1)
    ticks = np.linspace(0, T-1, 6)
    labels = [f"{int(round(x))}" for x in np.linspace(0, 100, 6)]
    ax.set_xticks(ticks); ax.set_xticklabels(labels)
    ax.set_xlabel("Gait cycle (%)")

def _draw_threshold_and_label(ax, spmi, T, stat_symbol='F', two_sided=False):
    zstar = float(getattr(spmi, 'zstar', np.nan))
    alpha_used = float(getattr(spmi, 'alpha', np.nan))
    if np.isfinite(zstar):
        ax.axhline(zstar, color='k', ls='--', lw=1, alpha=0.65)
        if two_sided: ax.axhline(-zstar, color='k', ls='--', lw=1, alpha=0.65)
        ax.text(0.5*(T-1), zstar, rf"Î± = {alpha_used:.6f};  {stat_symbol}$^*$ = {zstar:.3f}",
                ha="center", va="bottom", color="crimson", fontsize=10)
    return zstar

def _shade_and_label(ax, spans_df, T, zstar, stat_symbol='t'):
    if spans_df is None or spans_df.empty: return
    y0, y1 = ax.get_ylim(); yr = y1-y0
    for _, r in spans_df.iterrows():
        s_pct, e_pct = float(r["start_%"]), float(r["end_%"])
        qv = float(r.get("q_fdr", np.nan))
        peak = float(r.get("peak_stat", np.nan))
        i0 = (s_pct/100.0)*(T-1); i1 = (e_pct/100.0)*(T-1); xmid = 0.5*(i0+i1)
        if stat_symbol.lower() == 'f' or (np.isfinite(peak) and peak >= 0):
            face=(1.0,0.70,0.70,0.35); edge=(0.85,0.35,0.35); ytxt=zstar+0.12*yr
            va='bottom'
        else:
            face=(0.70,0.80,1.0,0.35); edge=(0.35,0.45,0.85); ytxt=-zstar-0.12*yr
            va='top'
        ax.axvspan(i0, i1, facecolor=face, edgecolor=edge, lw=1.0)
        ax.text(xmid, ytxt, _fmt_q(qv), ha="center", va=va, color="black", fontsize=9)

def plot_spm_fdr_with_q(group_signals, groups, omni_df, pair_df,
                        muscles=('RF','VM','VL','GM','GL'), alpha_global=ALPHA, fdr_alpha=FDR_ALPHA):
    for m in muscles:
        # ---------- OMNIBUS ----------
        y, A, G_arrays = _stack_wholecycle_for_plot(group_signals, groups, m, min_curves=MIN_CURVES_PER_GROUP)
        if y is not None and len(np.unique(A)) >= 2:
            T = y.shape[1]
            fig, ax = plt.subplots(figsize=FIGSIZE)
            plt.sca(ax); spmiF = _anova_spmi(y, A, alpha=alpha_global); spmiF.plot()
            ax = plt.gca(); ax.axhline(0, color='0.4', lw=0.8, ls=':')
            zstar = _draw_threshold_and_label(ax, spmiF, T, stat_symbol='F', two_sided=False)
            spans = omni_df.loc[(omni_df.get("MUSCLE","")==m) & (omni_df.get("q_fdr",1.1) <= fdr_alpha)].copy()
            _shade_and_label(ax, spans, T, zstar, stat_symbol='F')
            _xticks_percent(ax, T)
            ax.set_title(f"{m} | SPM{{F}} (nonparam) â€” FDR qâ‰¤{fdr_alpha:.2f} shown")
            plt.tight_layout(); plt.show()
        else:
            print(f"[omnibus] {m}: insufficient data")

        # ---------- PAIRWISE ----------
        if y is None: continue
        gps = [g for g in G_arrays.keys() if G_arrays[g] is not None]
        for gA, gB in itertools.combinations(gps, 2):
            T = y.shape[1]
            fig, ax = plt.subplots(figsize=FIGSIZE)
            plt.sca(ax)
            spmiT = spm1d.stats.nonparam.ttest2(G_arrays[gA], G_arrays[gB]).inference(alpha=alpha_global/len(list(itertools.combinations(gps, 2))), two_tailed=True, iterations=NP_ITER)
            spmiT.plot()
            ax = plt.gca(); ax.axhline(0, color='0.4', lw=0.8, ls=':')
            zstar = _draw_threshold_and_label(ax, spmiT, T, stat_symbol='t', two_sided=True)
            mask = (pair_df.get("MUSCLE","")==m) & pair_df.get("TEST","").str.contains(f"{gA} vs {gB}", regex=False)
            spans = pair_df.loc[mask & (pair_df.get("q_fdr",1.1) <= fdr_alpha)].copy()
            _shade_and_label(ax, spans, T, zstar, stat_symbol='t')
            _xticks_percent(ax, T)
            ax.set_title(f"{m} | {gA} vs {gB} | SPM{{t}} â€” FDR qâ‰¤{fdr_alpha:.2f} shown")
            plt.tight_layout(); plt.show()

# ----- RERUN the pipeline pieces with the new extractors -----
omni_df_raw, pair_df_raw = run_spm_wholecycle_nonparam(
    group_signals=group_signals, groups=groups, muscles=MUSCLES,
    save_path=None, make_plots=False  # no first-wave plots
)
omni_df, pair_df = apply_familywise_corrections(omni_df_raw, pair_df_raw, alpha=ALPHA)

# Visualize with q annotations
plot_spm_fdr_with_q(group_signals, groups, omni_df, pair_df,
                    muscles=('RF','VM','VL','GM','GL'),
                    alpha_global=ALPHA, fdr_alpha=0.05)


###================================================================================================================###
#==  SECTION 3  EMGâ€“Kinematics Analysis (tv-VAR + IRFs)
# ================================================================================================================###
#  EMGâ€“Kinematics Analysis (tv-VAR + IRFs)
#  - Subject ID from names (COGNOME+NOME)
#  - Trial detection (for QA only; no inference)
#  - Robust per-phase nearest-time merge
#  - Lighter tv-VAR basis, gentler gate
#  - XCorr, HAC OLS, Granger(+FDR) + IRF metrics (both dirs)
#  - NO subject-level inferential statistics
# ===========================================

import warnings
warnings.filterwarnings("ignore")

# =========================
# Imports
# =========================
import numpy as np
import pandas as pd
import statsmodels.api as sm
from statsmodels.tsa.stattools import grangercausalitytests
from statsmodels.stats.multitest import multipletests
from sklearn.preprocessing import StandardScaler, SplineTransformer
from sklearn.linear_model import RidgeCV
from IPython.display import display, HTML
import matplotlib.pyplot as plt
from pathlib import Path
from numpy.linalg import cholesky
import unicodedata

# =========================
# Config / Data I/O
# =========================
DATA_PATH = "sEMG data HSP/Timeseries completo.csv"   # kinematics CSV
DECIMAL = ","
DELIMITER = ";"

# Preprocessed EMG candidates (should include RF, VM, VL; names optional)
EMG_FILE_CANDIDATES = [
    "emg_preprocessed.parquet",
    "preprocessed_emg.parquet",
    "emg_preprocessed.csv",
    "preprocessed_emg.csv",
    "prova auto horizon knee only.csv",  # your uploaded file name
]

# Group mapping
EMG_GROUP_MAP = {"NORMATIVI": "NESSUNO", "GRUPPO1": "1", "GRUPPO2": "2", "GRUPPO3": "3"}

# Analysis knobs
MIN_POINTS = 8
HARD_CAP_MAX_LAGS = 8
FAST_MODE = True
SAVE_PLOTS = False
RANDOM_STATE = 42

# tv-VAR grids (lighter)
TVVAR_N_KNOTS_FAST = 2
TVVAR_N_KNOTS_FULL = 3
TVVAR_N_KNOTS = TVVAR_N_KNOTS_FAST if FAST_MODE else TVVAR_N_KNOTS_FULL
TVVAR_DEGREE = 1
TVVAR_ALPHAS_FAST = np.array([1e-3, 1e-2, 1e-1, 1, 10])
TVVAR_ALPHAS_FULL = np.logspace(-6, 2, 15)
TVVAR_ALPHAS = TVVAR_ALPHAS_FAST if FAST_MODE else TVVAR_ALPHAS_FULL
TVVAR_P_MAX_DEFAULT = 1

DEFAULT_STRIDE_MS = 1000  # gait cycle ~1s

# =========================
# Variable sets
# =========================
emg_vars = ["RF", "VL", "VM"]

KNEE_KIN_VARS = [
    "Angolo_Flex-ext ginocchio",
    "Vel_Flex-ext ginocchio",
    "Acc_Flex-ext ginocchio",
]
HIP_KIN_VARS = [
    "Angolo_Flex-ext anca",
    "Vel_Flex-ext anca"
    "Acc_Flex-ext anca",
]
kin_vars = KNEE_KIN_VARS 

# Column names
GROUP_COL = "SOTTOGRUPPO"
PHASE_COL = "PHASE"
TIME_COL  = "Percentage"
LASTNAME_COL  = "COGNOME"
FIRSTNAME_COL = "NOME"
SUBJECT_COL   = "SOGGETTO"
TRIAL_COL     = "TRIAL_ID"

# =========================
# Helpers (display, stats)
# =========================
def _safe_z(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x, dtype=float)
    m = np.nanmean(x); s = np.nanstd(x)
    if not np.isfinite(s) or s == 0: return np.zeros_like(x, dtype=float)
    return (x - m) / s

def _rolling_smooth(arr, window: int | None):
    if not window or window <= 1: return np.asarray(arr, dtype=float)
    a = pd.Series(arr, dtype="float64")
    return a.rolling(window=window, min_periods=max(1, window // 3), center=True).mean().to_numpy()

def _pearson_xcorr(y, x, max_lag: int | None = None):
    """XCorr of z-scored y vs x; lag>0 â‡’ y lags x (y=EMG, x=KIN)."""
    y = _safe_z(np.asarray(y, dtype=float)); x = _safe_z(np.asarray(x, dtype=float))
    n = min(len(x), len(y)); x, y = x[:n], y[:n]
    m = np.isfinite(x) & np.isfinite(y); x, y = x[m], y[m]
    if len(x) < 3: return np.array([], dtype=int), np.array([], dtype=float)
    corr_full = np.correlate(y, x, mode="full") / len(x)
    lags = np.arange(-len(x) + 1, len(x))
    if max_lag is not None:
        keep = (lags >= -max_lag) & (lags <= max_lag)
        lags, corr_full = lags[keep], corr_full[keep]
    return lags, corr_full

def scroll_table(title, df, note=""):
    html = f"""
    <style>
    .scroll-table {{ display:block; max-height:600px; overflow-y:auto; overflow-x:auto; white-space:nowrap; border:1px solid #ccc; padding:10px; margin-bottom:20px; font-family:monospace; }}
    table {{ border-collapse: collapse; }}
    th, td {{ padding:4px 8px; border-bottom:1px solid #ddd; }}
    .caption-note {{ color:#555; font-style:italic; margin-top:-8px; margin-bottom:8px; }}
    </style>
    <h3>{title}</h3>
    <div class='caption-note'>{note}</div>
    <div class='scroll-table'>{df.to_html(index=False)}</div>
    """
    display(HTML(html))

def hac_maxlags(n, p=None):
    root = int(np.sqrt(max(n, 1)) // 2)
    return max(1, (p if p is not None else 0), root)

def compute_pmax(n_obs, k=2, hard_cap=HARD_CAP_MAX_LAGS):
    p_df = int((n_obs - 1) / (k + 1)) - 1
    if p_df < 1: p_df = 1
    p_schwert = int(np.floor(12 * (max(n_obs, 1) / 100.0) ** 0.25))
    p_schwert = max(1, p_schwert)
    return int(max(1, min(hard_cap, p_df, p_schwert)))

def xcorr_maxlag(n_obs):
    return max(5, min(int(n_obs // 3), int(2 * np.sqrt(max(n_obs, 1)))))

# =========================
# Load kinematics
# =========================
PREFERRED_PHASE_ORDER = ["IC","LR","MSt","TSt","PSw","ISw","MSw","TSw"]

def _normalize_group(s):
    s = str(s).strip().upper()
    remap = {"NORMATIVI":"NESSUNO","GRUPPO1":"1","GRUPPO2":"2","GRUPPO3":"3"}
    s = remap.get(s, s)
    try:
        v = float(s.replace(",", "."))
        if np.isfinite(v):
            return str(int(v)) if v.is_integer() else str(v).rstrip("0").rstrip(".")
    except Exception:
        pass
    return s

df_raw = pd.read_csv(DATA_PATH, delimiter=DELIMITER, decimal=DECIMAL)
df_raw[TIME_COL]  = pd.to_numeric(df_raw[TIME_COL], errors='coerce')
df_raw[GROUP_COL] = df_raw[GROUP_COL].map(_normalize_group)
df_raw[PHASE_COL] = df_raw[PHASE_COL].astype(str).str.strip()

raw_phases = [p for p in PREFERRED_PHASE_ORDER if p in set(df_raw[PHASE_COL].dropna().unique())]
others     = [p for p in pd.Series(df_raw[PHASE_COL].dropna()).unique() if p not in raw_phases]
phase_labels = raw_phases + others
print("ğŸ—‚ Phase labels in use:", phase_labels)

# =========================
# Load EMG (preprocessed) or from in-memory group_signals
# =========================
def try_load_emg_file() -> pd.DataFrame | None:
    for path in EMG_FILE_CANDIDATES:
        p = Path(path)
        if not p.exists(): 
            continue
        if p.suffix.lower() == ".parquet":
            df = pd.read_parquet(p)
        elif p.suffix.lower() == ".csv":
            txt = open(p, "r", encoding="utf-8", errors="ignore").read(1024)
            if ";" in txt:
                df = pd.read_csv(p, delimiter=";", decimal=",")
            else:
                df = pd.read_csv(p)
        else:
            continue
        need_emg = {GROUP_COL, PHASE_COL, TIME_COL, *emg_vars}
        if not need_emg.issubset(df.columns):
            continue
        df = df.copy()
        df[GROUP_COL] = df[GROUP_COL].astype(str).str.strip().map(lambda s: EMG_GROUP_MAP.get(s, s))
        df[PHASE_COL] = df[PHASE_COL].astype(str).str.strip()
        df[TIME_COL]  = pd.to_numeric(df[TIME_COL], errors="coerce")
        # pass through names if present
        for c in [LASTNAME_COL, FIRSTNAME_COL, SUBJECT_COL]:
            if c in df.columns:
                df[c] = df[c].astype(str).str.strip()
        return df
    return None

def emg_from_group_signals(group_signals: dict, phase_labels: list[str]) -> pd.DataFrame:
    muscles = list(next(iter(group_signals.values())).keys())
    n_phases = len(phase_labels)
    rows = []
    for g, mdict in group_signals.items():
        g_norm = EMG_GROUP_MAP.get(str(g).strip(), str(g).strip())
        for p_idx in range(n_phases):
            lengths = []
            for m in muscles:
                arrs = mdict[m][p_idx]
                lengths.extend([len(a) for a in arrs if isinstance(a, (list, np.ndarray)) and len(a) >= 3])
            if not lengths: continue
            L = int(min(lengths))
            means = {}
            for m in muscles:
                arrs = mdict[m][p_idx]
                if not arrs: means[m] = None; continue
                stack = np.vstack([np.asarray(a)[:L] for a in arrs if isinstance(a, (list, np.ndarray)) and len(a) >= L])
                means[m] = np.nanmean(stack, axis=0) if stack.size else None
            pct = np.linspace(0, 100, L)
            for i in range(L):
                row = {GROUP_COL: g_norm, PHASE_COL: phase_labels[p_idx], TIME_COL: float(pct[i])}
                for m in muscles:
                    row[m] = float(means[m][i]) if means[m] is not None else np.nan
                rows.append(row)
    df = pd.DataFrame(rows)
    for col in ["RF","VL","VM"]:
        if col not in df.columns: df[col] = np.nan
    df[GROUP_COL] = df[GROUP_COL].astype(str).str.strip()
    df[PHASE_COL] = df[PHASE_COL].astype(str).str.strip()
    df[TIME_COL]  = pd.to_numeric(df[TIME_COL], errors="coerce")
    return df

# Build EMG
if "group_signals" in globals() and isinstance(globals()["group_signals"], dict):
    avg_emg = emg_from_group_signals(group_signals, phase_labels=phase_labels)
    print("âœ… Using EMG from in-memory `group_signals` (group-mean).")
else:
    avg_emg = try_load_emg_file()
    if avg_emg is None or avg_emg.empty:
        raise RuntimeError("No preprocessed EMG found. Provide [SOTTOGRUPPO, PHASE, Percentage, RF, VL, VM].")
    print("âœ… Using preprocessed EMG from file.")

# =========================
# SUBJECT ID from names + TRIAL detection (no inference)
# =========================
POSSIBLE_LASTNAME_COLS = ["COGNOME","Cognome","cognome","LASTNAME","LastName","Surname","SURNAME"]
POSSIBLE_FIRSTNAME_COLS = ["NOME","Nome","nome","FIRSTNAME","FirstName","GivenName","GIVENNAME"]

def _norm_name(s):
    s = str(s) if pd.notna(s) else ""
    s = s.strip().upper()
    s = "".join(c for c in unicodedata.normalize("NFKD", s) if not unicodedata.combining(c))
    s = " ".join(s.split())
    return s

def _find_col(df, candidates):
    for c in candidates:
        if c in df.columns: return c
    return None

def ensure_subject_from_names(df, out_col="SOGGETTO"):
    df = df.copy()
    ln_col = _find_col(df, POSSIBLE_LASTNAME_COLS)
    fn_col = _find_col(df, POSSIBLE_FIRSTNAME_COLS)
    if ln_col and fn_col:
        ln = df[ln_col].map(_norm_name)
        fn = df[fn_col].map(_norm_name)
        df[out_col] = (ln + "_" + fn).str.replace(r"[^A-Z0-9_]+", "", regex=True)
    return df

def assign_trials(df, by_cols, time_col=TIME_COL, out_col="TRIAL_ID"):
    df = df.copy()
    if not set(by_cols + [time_col]).issubset(df.columns):
        df[out_col] = 0
        return df
    df[time_col] = pd.to_numeric(df[time_col], errors="coerce").astype(float)
    df = df.sort_values(by_cols + [time_col], kind="mergesort").reset_index(drop=True)
    def _trial_id(s):
        dif = s.diff().fillna(0.0)
        return (dif < 0).astype(int).cumsum().astype(int)
    df[out_col] = (
        df.groupby(by_cols, group_keys=False)[time_col]
          .apply(_trial_id).astype(int)
    )
    return df

df_raw = ensure_subject_from_names(df_raw, out_col=SUBJECT_COL)
avg_emg = ensure_subject_from_names(avg_emg, out_col=SUBJECT_COL)

if SUBJECT_COL in df_raw.columns:
    df_raw = assign_trials(df_raw, by_cols=[GROUP_COL, PHASE_COL, SUBJECT_COL], time_col=TIME_COL, out_col=TRIAL_COL)
else:
    df_raw = assign_trials(df_raw, by_cols=[GROUP_COL, PHASE_COL], time_col=TIME_COL, out_col=TRIAL_COL)

if SUBJECT_COL in avg_emg.columns:
    avg_emg = assign_trials(avg_emg, by_cols=[GROUP_COL, PHASE_COL, SUBJECT_COL], time_col=TIME_COL, out_col=TRIAL_COL)
else:
    avg_emg = assign_trials(avg_emg, by_cols=[GROUP_COL, PHASE_COL], time_col=TIME_COL, out_col=TRIAL_COL)

# =========================
# Robust per-phase nearest-time merge (subject-aware when possible)
# =========================
def _ensure_asof_ready(df, by_cols, time_col):
    df = df.copy()
    df[time_col] = pd.to_numeric(df[time_col], errors="coerce")
    df = df.dropna(subset=by_cols + [time_col])
    df[time_col] = df[time_col].astype(float)
    df = df.sort_values(by_cols + [time_col], kind="mergesort").reset_index(drop=True)
    return df

def merge_phasewise_robust(EMG, KIN, time_col="Percentage", tolerance=0.5):
    # Prefer subject-level if present in BOTH
    if (SUBJECT_COL in EMG.columns) and EMG[SUBJECT_COL].notna().any() and \
       (SUBJECT_COL in KIN.columns) and KIN[SUBJECT_COL].notna().any():
        by_cols = [GROUP_COL, PHASE_COL, SUBJECT_COL]
    else:
        by_cols = [GROUP_COL, PHASE_COL]

    left  = _ensure_asof_ready(EMG, by_cols, time_col)
    right = _ensure_asof_ready(KIN, by_cols, time_col)
    right[time_col] = right[time_col].astype(left[time_col].dtype)

    try:
        out = pd.merge_asof(
            left.sort_values(by_cols + [time_col], kind="mergesort"),
            right.sort_values(by_cols + [time_col], kind="mergesort"),
            on=time_col, by=by_cols, direction="nearest",
            tolerance=float(tolerance)
        )
        return out
    except ValueError as e:
        if "keys must be sorted" not in str(e):
            raise
        pieces = []
        left_groups  = set(tuple(k) for k in left.groupby(by_cols).groups.keys())
        right_groups = set(tuple(k) for k in right.groupby(by_cols).groups.keys())
        common = sorted(left_groups & right_groups)
        for key in common:
            if not isinstance(key, tuple): key = (key,)
            mask_left = np.logical_and.reduce([left[c].astype(str)==str(v) for c, v in zip(by_cols, key)])
            mask_right = np.logical_and.reduce([right[c].astype(str)==str(v) for c, v in zip(by_cols, key)])
            L = left.loc[mask_left].sort_values(time_col, kind="mergesort")
            R = right.loc[mask_right].sort_values(time_col, kind="mergesort")
            if len(L)==0 or len(R)==0: continue
            sub = pd.merge_asof(L, R, on=time_col, direction="nearest", tolerance=float(tolerance))
            for c, v in zip(by_cols, key): sub[c] = v
            pieces.append(sub)
        if not pieces:
            return left.iloc[0:0].merge(right.iloc[0:0], how="left", on=[time_col], suffixes=("", "_y"))
        out = pd.concat(pieces, ignore_index=True).sort_values(by_cols + [time_col], kind="mergesort").reset_index(drop=True)
        return out

# Prepare and merge
kin_keep = [GROUP_COL, PHASE_COL, TIME_COL, SUBJECT_COL, TRIAL_COL] + kin_vars
for c in [SUBJECT_COL, TRIAL_COL]:
    if c not in df_raw.columns: df_raw[c] = np.nan
df_raw = df_raw[[c for c in kin_keep if c in df_raw.columns]].copy()

emg_keep = [GROUP_COL, PHASE_COL, TIME_COL, SUBJECT_COL, TRIAL_COL] + emg_vars
avg_emg = avg_emg[[c for c in emg_keep if c in avg_emg.columns]].copy()

avg_df = merge_phasewise_robust(avg_emg, df_raw, time_col=TIME_COL, tolerance=0.5)
avg_df = avg_df.dropna(subset=[GROUP_COL, PHASE_COL, TIME_COL]).reset_index(drop=True)
print("âœ… Per-phase tolerant merge (robust). Shape:", avg_df.shape)

# Keep only available EMG channels
available_emg = [m for m in emg_vars if m in avg_df.columns and avg_df[m].notna().any()]
if set(available_emg) != set(emg_vars):
    missing = list(set(emg_vars) - set(available_emg))
    if missing: print("âš ï¸ Skipping missing EMG channels:", missing)
emg_vars = available_emg

# =========================
# tv-VAR core (spline basis + ridge)
# =========================
def build_spline_basis(perc_array, n_knots=TVVAR_N_KNOTS, degree=TVVAR_DEGREE):
    tau = (np.asarray(perc_array, dtype=float) / 100.0).reshape(-1,1)
    st = SplineTransformer(n_knots=n_knots, degree=degree, include_bias=True)
    B = st.fit_transform(tau)
    return B, st

def eval_basis(st, perc_scalar):
    tau = np.array([[float(perc_scalar)/100.0]])
    return st.transform(tau).ravel()

def tvvar_design(kin, emg, perc, p, st):
    k = 2
    B = st.transform((np.asarray(perc)/100.0).reshape(-1,1))
    R = B.shape[1]
    kin = np.asarray(kin, float); emg = np.asarray(emg, float)
    n = len(kin)
    rows = []; Y = []
    for t in range(p, n):
        Bt = B[t]
        Y.append([kin[t], emg[t]])
        feats = [Bt]
        for L in range(1, p+1):
            xlag = np.array([kin[t-L], emg[t-L]])
            feats.append(np.outer(xlag, Bt).reshape(-1))
        rows.append(np.concatenate(feats))
    Y = np.array(Y)
    X = np.vstack(rows)
    return Y, X, {"R": R, "k": 2, "p": p}

def fit_tvvar_ridge(kin, emg, perc, p, alphas=TVVAR_ALPHAS, n_knots=TVVAR_N_KNOTS, degree=TVVAR_DEGREE):
    B, st = build_spline_basis(perc, n_knots=n_knots, degree=degree)
    Y, X, meta = tvvar_design(kin, emg, perc, p, st)
    R = meta["R"]; k = meta["k"]
    models = []; coefs = []; b0 = []
    for eq in range(k):
        y = Y[:, eq]
        ridge = RidgeCV(alphas=alphas, fit_intercept=False)
        ridge.fit(X, y)
        beta = ridge.coef_.copy()
        off = 0
        b0_eq = beta[off:off+R]; off += R
        blocks = []
        for L in range(1, p+1):
            blk = beta[off:off+k*R].reshape(k, R); blocks.append(blk); off += k*R
        models.append(ridge); b0.append(b0_eq); coefs.append(blocks)
    Yhat = np.column_stack([models[0].predict(X), models[1].predict(X)])
    resid = Y - Yhat
    Sigma_u = np.cov(resid.T)
    return {"st": st, "R": R, "k": k, "p": p, "coefs": coefs, "b0": b0, "Sigma_u": Sigma_u, "resid": resid}

def A_of_tau(model, perc_scalar):
    st = model["st"]; p = model["p"]; k = model["k"]; R = model["R"]
    Bt = eval_basis(st, perc_scalar)
    A_list = []
    for L in range(p):
        A_L = np.zeros((k, k))
        for eq in range(k):
            blk = model["coefs"][eq][L]
            A_L[eq, :] = blk @ Bt
        A_list.append(A_L)
    return A_list

def companion_from_Alist(A_list):
    p = len(A_list); k = A_list[0].shape[0]
    F = np.zeros((k*p, k*p))
    F[:k, :k*p] = np.hstack(A_list)
    if p > 1: F[k:, :-k] = np.eye(k*(p-1))
    return F

def generalized_impulse_vector(Sigma_u, s):
    e_s = np.zeros(Sigma_u.shape[0]); e_s[s] = 1.0
    denom = np.sqrt(max(Sigma_u[s, s], 1e-12))
    return (Sigma_u @ e_s) / denom

def tvvar_irf(model, perc_start, step_pct, H=10, shock_index=1, orth="generalized"):
    k = model["k"]; p = model["p"]
    Z = np.zeros((k*p,))
    if orth == "chol":
        L = cholesky(model["Sigma_u"]); g = L[:, shock_index]
    else:
        g = generalized_impulse_vector(model["Sigma_u"], shock_index)
    irf = [g.copy()]; Z[:k] += g
    perc = float(perc_start)
    for h in range(1, H+1):
        perc = perc + step_pct
        if perc > 100: perc -= 100
        A_list = A_of_tau(model, perc)
        F = companion_from_Alist(A_list)
        Z = F @ Z
        irf.append(Z[:k].copy())
    return np.vstack(irf)

def tvvar_fevd_shares(model, perc_start, step_pct, H=10):
    k = model["k"]
    steps = np.arange(1, H+1)
    irfs = [tvvar_irf(model, perc_start, step_pct, H=H, shock_index=s, orth="generalized") for s in range(k)]
    contrib = np.stack([arr[1:, :] for arr in irfs], axis=2)  # (H,k,k)
    num = contrib**2; den = np.sum(num, axis=2, keepdims=True) + 1e-12
    shares = num / den
    return steps, shares

# =========================
# Horizon helpers
# =========================
HORIZON_CAP_DEFAULT = 60
ENERGY_STOP_DEFAULT = 0.95
FEVD_TOL_DEFAULT    = 0.01
FEVD_WINDOW_STEPS   = 3
FIRSTZERO_TAIL      = 2

def _infer_step_pct(perc: np.ndarray) -> float:
    return float(np.nanmedian(np.diff(perc))) if len(perc) > 1 else 1.0

def _subphase_horizon_from_perc(perc: np.ndarray, step_pct: float, cap: int = HORIZON_CAP_DEFAULT, min_steps: int = 2) -> int:
    if len(perc) < 2: return max(min_steps, 2)
    phase_len_pct = float(np.nanmax(perc) - np.nanmin(perc))
    H = int(np.ceil(max(step_pct, phase_len_pct) / max(step_pct, 1e-9)))
    return int(max(min_steps, min(H, cap)))

def _energy_truncate(y: np.ndarray, frac: float, cap: int = HORIZON_CAP_DEFAULT, min_steps: int = 2) -> int:
    y = np.asarray(y, float)
    if y.size == 0: return min_steps
    e = np.cumsum(y**2); tot = float(e[-1])
    if not np.isfinite(tot) or tot <= 0: return min(min_steps, len(y)-1)
    idx = int(np.searchsorted(e, frac * tot))
    H = max(min_steps, min(idx, len(y)-1, cap))
    return H

def _first_zero_horizon(y: np.ndarray, tail: int = FIRSTZERO_TAIL, cap: int = HORIZON_CAP_DEFAULT, min_steps: int = 2) -> int:
    y = np.asarray(y, float)
    if y.size < 2: return min_steps
    s = np.sign(y)
    zc = np.where(s[1:] * s[:-1] <= 0)[0]
    if len(zc) == 0: return min(len(y)-1, cap)
    H = int(zc[0] + 1 + max(0, int(tail)))
    return max(min_steps, min(H, len(y)-1, cap))

def _fevd_plateau_horizon(model: dict, perc_start: float, step_pct: float, H_cap: int, resp_idx: int, shock_idx: int,
                          tol: float = FEVD_TOL_DEFAULT, w: int = FEVD_WINDOW_STEPS) -> int:
    steps, shares = tvvar_fevd_shares(model, perc_start, step_pct, H=H_cap)
    s = shares[:, resp_idx, shock_idx]
    H = len(s)
    if H <= w+1: return H
    for h in range(w+1, H+1):
        win = s[h-w-1:h-1]
        if np.all(np.isfinite(win)) and (np.max(win) - np.min(win)) < tol:
            return h
    return H

def choose_optimal_horizon(model: dict, perc: np.ndarray, step_pct: float, *, direction: str, mode: str = 'auto_optimal',
                           energy_stop: float = ENERGY_STOP_DEFAULT, horizon_cap: int = HORIZON_CAP_DEFAULT) -> int:
    subphase_cap = _subphase_horizon_from_perc(perc, step_pct, cap=horizon_cap, min_steps=2)
    H_cap = min(subphase_cap, horizon_cap)
    if direction == 'EMGâ†’KIN':
        arr = tvvar_irf(model, perc_start=float(perc[0]), step_pct=step_pct, H=H_cap, shock_index=1, orth="generalized"); resp_idx = 0; shock_idx=1
    elif direction == 'KINâ†’EMG':
        arr = tvvar_irf(model, perc_start=float(perc[0]), step_pct=step_pct, H=H_cap, shock_index=0, orth="generalized"); resp_idx = 1; shock_idx=0
    else:
        raise ValueError("direction must be 'EMGâ†’KIN' or 'KINâ†’EMG'")
    y = arr[:, resp_idx]
    m = mode.lower()
    if m == 'subphase': return H_cap
    if m == 'firstzero': return _first_zero_horizon(y, tail=FIRSTZERO_TAIL, cap=H_cap, min_steps=2)
    if m == 'energy': return _energy_truncate(y, frac=float(energy_stop), cap=H_cap, min_steps=2)
    if m == 'fevd_plateau':
        return _fevd_plateau_horizon(model, float(perc[0]), step_pct, H_cap, resp_idx=resp_idx, shock_idx=shock_idx, tol=FEVD_TOL_DEFAULT, w=FEVD_WINDOW_STEPS)
    H_energy = _energy_truncate(y, frac=float(energy_stop), cap=H_cap, min_steps=2)
    H_fevd   = _fevd_plateau_horizon(model, float(perc[0]), step_pct, H_cap, resp_idx=resp_idx, shock_idx=shock_idx, tol=FEVD_TOL_DEFAULT, w=FEVD_WINDOW_STEPS)
    return max(2, min(H_energy, H_fevd, H_cap))

# =========================
# IRF metrics + selection
# =========================
def _irf_summary_stats(y: np.ndarray, step_pct: float, stride_ms: float) -> dict:
    y = np.asarray(y, float); n = len(y)
    steps = np.arange(n, dtype=int)
    pct = steps * float(step_pct)
    ms = (pct / 100.0) * float(stride_ms)
    if n == 0 or not np.isfinite(y).any():
        return {'H': 0, 'step_pct': float(step_pct),
                'peak_pos': np.nan, 'peak_pos_step': np.nan, 'peak_pos_%gait': np.nan, 'peak_pos_ms': np.nan,
                'peak_neg': np.nan, 'peak_neg_step': np.nan, 'peak_neg_%gait': np.nan, 'peak_neg_ms': np.nan,
                'max_abs': np.nan, 'max_abs_step': np.nan, 'max_abs_%gait': np.nan, 'max_abs_ms': np.nan,
                'area': np.nan, 'area_pos': np.nan, 'area_neg': np.nan, 'energy': np.nan,
                'duration_pos_steps': np.nan, 'duration_neg_steps': np.nan,
                'zero_crossings': np.nan, 'first_cross_step': np.nan, 'first_cross_%gait': np.nan, 'first_cross_ms': np.nan,
                'long_run': np.nan}
    peak_pos_step = int(np.nanargmax(y)); peak_neg_step = int(np.nanargmin(y))
    peak_pos = float(y[peak_pos_step]);   peak_neg = float(y[peak_neg_step])
    max_abs_step = int(np.nanargmax(np.abs(y))); max_abs = float(y[max_abs_step])
    area = float(np.nansum(y))
    area_pos = float(np.nansum(y[y > 0])) if np.any(y > 0) else 0.0
    area_neg = float(np.nansum(y[y < 0])) if np.any(y < 0) else 0.0
    energy = float(np.nansum(y**2))
    duration_pos_steps = int(np.sum(y > 0))
    duration_neg_steps = int(np.sum(y < 0))
    sign = np.sign(y)
    zc_idx = np.where(sign[1:] * sign[:-1] < 0)[0]
    zero_crossings = int(len(zc_idx))
    if zero_crossings > 0:
        first_cross_step = int(zc_idx[0] + 1)
        first_cross_pct  = float(pct[first_cross_step])
        first_cross_ms   = float(ms[first_cross_step])
    else:
        first_cross_step = np.nan; first_cross_pct = np.nan; first_cross_ms = np.nan
    return {
        'H': int(n - 1), 'step_pct': float(step_pct),
        'peak_pos': peak_pos, 'peak_pos_step': peak_pos_step, 'peak_pos_%gait': float(pct[peak_pos_step]), 'peak_pos_ms': float(ms[peak_pos_step]),
        'peak_neg': peak_neg, 'peak_neg_step': peak_neg_step, 'peak_neg_%gait': float(pct[peak_neg_step]), 'peak_neg_ms': float(ms[peak_neg_step]),
        'max_abs': max_abs, 'max_abs_step': max_abs_step, 'max_abs_%gait': float(pct[max_abs_step]), 'max_abs_ms': float(ms[max_abs_step]),
        'area': area, 'area_pos': area_pos, 'area_neg': area_neg, 'energy': energy,
        'duration_pos_steps': duration_pos_steps, 'duration_neg_steps': duration_neg_steps,
        'zero_crossings': zero_crossings, 'first_cross_step': first_cross_step, 'first_cross_%gait': first_cross_pct, 'first_cross_ms': first_cross_ms,
        'long_run': area,
    }

# Gentler tv-VAR model selection
def _select_tvvar_for_pair(kin_v: np.ndarray, emg_v: np.ndarray, perc: np.ndarray):
    n_obs = len(kin_v)
    pmax = min(TVVAR_P_MAX_DEFAULT, compute_pmax(n_obs, 2, HARD_CAP_MAX_LAGS))
    p_grid = [p for p in range(1, pmax + 1) if (n_obs - p) >= 8]
    best_p = None; best_score = np.inf; best_model = None
    for p in p_grid:
        try:
            m = fit_tvvar_ridge(kin_v, emg_v, perc, p)
            R = m['R']; kdim = m['k']
            if (n_obs - p) < max(8, 1 * kdim * R):
                continue
            resid = m['resid']; sig = np.mean(np.sum(resid**2, axis=1))
            kparams = kdim * (R + p * kdim * R)
            sc = np.log(sig + 1e-9) + 2 * kparams / max(len(resid), 10)
            if sc < best_score:
                best_score, best_p, best_model = sc, p, m
        except Exception:
            continue
    return best_model, (best_p if best_p is not None else 1)

def _compute_irf_for_direction(model: dict, perc_start: float, step_pct: float, H: int, direction: str, orth: str):
    if direction == 'EMGâ†’KIN':
        arr = tvvar_irf(model, perc_start=perc_start, step_pct=step_pct, H=H, shock_index=1, orth=orth); resp_idx = 0
    elif direction == 'KINâ†’EMG':
        arr = tvvar_irf(model, perc_start=perc_start, step_pct=step_pct, H=H, shock_index=0, orth=orth); resp_idx = 1
    else:
        raise ValueError("direction must be 'EMGâ†’KIN' or 'KINâ†’EMG'")
    return arr[:, resp_idx]

def add_irf_metrics(merged_df: pd.DataFrame, results_df: pd.DataFrame, *,
                    alpha: float = 0.05, use_fdr: bool = True,
                    directions: str = 'all', orth: str = 'generalized',
                    horizon: int | str = 'auto_optimal', stride_ms: float = DEFAULT_STRIDE_MS,
                    max_pairs: int | None = None) -> pd.DataFrame:
    if results_df is None or results_df.empty:
        return results_df
    out_rows = []; processed = 0
    key_cols = ['Group', 'Phase', 'Muscle', 'KinematicVar']
    for _, r in results_df.iterrows():
        g, ph, emg, kin = r['Group'], r['Phase'], r['Muscle'], r['KinematicVar']
        if emg not in merged_df.columns or kin not in merged_df.columns: continue
        sub = merged_df.loc[(merged_df[GROUP_COL]==g) & (merged_df[PHASE_COL]==ph), [TIME_COL, emg, kin]].dropna()
        if len(sub) < MIN_POINTS: continue
        sub = sub.sort_values(TIME_COL)
        perc = sub[TIME_COL].to_numpy()
        kin_v = sub[kin].to_numpy().astype(float)
        emg_v = sub[emg].to_numpy().astype(float)
        dir_list = ['EMGâ†’KIN', 'KINâ†’EMG'] if directions in ('all','both_if_sig') else []
        if directions == 'significant':
            q_e2k = r.get('q_G_EMGâ†’KIN', np.nan); p_e2k = r.get('Granger_EMGâ†’KIN', np.nan)
            q_k2e = r.get('q_G_KINâ†’EMG', np.nan); p_k2e = r.get('Granger_KINâ†’EMG', np.nan)
            sig_e2k = (pd.notna(q_e2k) and q_e2k < alpha) if use_fdr else (pd.notna(p_e2k) and p_e2k < alpha)
            sig_k2e = (pd.notna(q_k2e) and q_k2e < alpha) if use_fdr else (pd.notna(p_k2e) and p_k2e < alpha)
            dir_list = (['EMGâ†’KIN'] if sig_e2k else []) + (['KINâ†’EMG'] if sig_k2e else [])
        if not dir_list: continue
        model, p_sel = _select_tvvar_for_pair(kin_v, emg_v, perc)
        if model is None: continue
        step_pct = _infer_step_pct(perc)
        def _H_for(d):
            if isinstance(horizon, str):
                if horizon.lower() == 'auto': return min(2*int(p_sel), HORIZON_CAP_DEFAULT)
                return choose_optimal_horizon(model, perc, step_pct, direction=d, mode=horizon.lower(),
                                              energy_stop=ENERGY_STOP_DEFAULT, horizon_cap=HORIZON_CAP_DEFAULT)
            return int(min(int(horizon), HORIZON_CAP_DEFAULT))
        for d in dir_list:
            H = _H_for(d)
            y_irf = _compute_irf_for_direction(model, perc_start=perc[0], step_pct=step_pct, H=H, direction=d, orth=orth)
            stats = _irf_summary_stats(y_irf, step_pct=step_pct, stride_ms=stride_ms)
            pref = f"IRF_{d}_"
            row = {c: val for c, val in zip(key_cols, [g, ph, emg, kin])}
            row.update({'direction': d, 'p_selected': int(p_sel),
                        f'{pref}H_chosen_steps': int(H),
                        f'{pref}H_chosen_ms': float(H * (step_pct/100.0) * stride_ms)})
            row.update({pref + k: v for k, v in stats.items()})
            out_rows.append(row)
        processed += 1
        if max_pairs is not None and processed >= max_pairs: break
    if not out_rows: return results_df
    metrics_df = pd.DataFrame(out_rows)
    merged = results_df.copy()
    for _, mrow in metrics_df.iterrows():
        mask = ((merged['Group']==mrow['Group']) & (merged['Phase']==mrow['Phase']) &
                (merged['Muscle']==mrow['Muscle']) & (merged['KinematicVar']==mrow['KinematicVar']))
        for col, val in mrow.items():
            if col in ['Group','Phase','Muscle','KinematicVar','direction']: continue
            if col not in merged.columns: merged[col] = np.nan
            merged.loc[mask, col] = val
    return merged

# =========================
# Main analysis loop (XCorr / HAC / Granger)
# =========================
results = []
for (group, phase), phase_df in avg_df.groupby([GROUP_COL, PHASE_COL]):
    for emg in emg_vars:
        for kin in kin_vars:
            if emg not in phase_df.columns or kin not in phase_df.columns: 
                continue
            sub = phase_df[[TIME_COL, emg, kin]].dropna()
            if len(sub) < MIN_POINTS: continue
            sub = sub.sort_values(TIME_COL)
            perc = sub[TIME_COL].to_numpy()
            kin_series = sub[kin].to_numpy().astype(float)
            emg_series = sub[emg].to_numpy().astype(float)
            n_obs = len(sub)
            # HAC OLS
            def hac_ols(y, x):
                X = sm.add_constant(x)
                try:
                    res = sm.OLS(y, X).fit(cov_type="HAC", cov_kwds={"maxlags": hac_maxlags(len(y))})
                    return float(res.params[1]), float(res.pvalues[1]), float(res.rsquared)
                except Exception:
                    return np.nan, np.nan, np.nan
            _, p_k2e, r2_k2e = hac_ols(emg_series, kin_series)
            _, p_e2k, r2_e2k = hac_ols(kin_series, emg_series)
            # standardized betas
            scaler = StandardScaler()
            try:
                kin_std = scaler.fit_transform(kin_series.reshape(-1,1)).ravel()
                emg_std = scaler.fit_transform(emg_series.reshape(-1,1)).ravel()
                b_k2e = sm.OLS(emg_std, sm.add_constant(kin_std)).fit(cov_type="HAC", cov_kwds={"maxlags": hac_maxlags(n_obs)}).params[1]
                b_e2k = sm.OLS(kin_std, sm.add_constant(emg_std)).fit(cov_type="HAC", cov_kwds={"maxlags": hac_maxlags(n_obs)}).params[1]
            except Exception:
                b_k2e = np.nan; b_e2k = np.nan
            # xcorr
            ml = xcorr_maxlag(n_obs)
            lags, cc = _pearson_xcorr(y=emg_series, x=kin_series, max_lag=ml)
            lag_peak = int(lags[np.nanargmax(np.abs(cc))]) if len(cc) else np.nan
            # Granger (2-var)
            min_p_k2e = np.nan; min_p_e2k = np.nan
            try:
                df_xy = pd.DataFrame({"kin": kin_series, "emg": emg_series}).dropna()
                if len(df_xy) >= MIN_POINTS:
                    p_upper = compute_pmax(len(df_xy), k=2, hard_cap=HARD_CAP_MAX_LAGS)
                    if FAST_MODE: p_upper = min(p_upper, 6)
                    p_upper = max(1, min(p_upper, len(df_xy)//4))
                    g1 = grangercausalitytests(df_xy[["emg","kin"]], maxlag=p_upper, verbose=False)
                    g2 = grangercausalitytests(df_xy[["kin","emg"]], maxlag=p_upper, verbose=False)
                    min_p_k2e = float(min(v[0]['ssr_ftest'][1] for v in g1.values()))
                    min_p_e2k = float(min(v[0]['ssr_ftest'][1] for v in g2.values()))
            except Exception:
                pass
            results.append({
                'Group': group, 'Phase': phase, 'Muscle': emg, 'KinematicVar': kin,
                'Lag_XCorr': lag_peak,
                'CoefStd_KINâ†’EMG': float(b_k2e) if np.isfinite(b_k2e) else np.nan,
                'CoefStd_EMGâ†’KIN': float(b_e2k) if np.isfinite(b_e2k) else np.nan,
                'R2_KINâ†’EMG': float(r2_k2e) if np.isfinite(r2_k2e) else np.nan,
                'R2_EMGâ†’KIN': float(r2_e2k) if np.isfinite(r2_e2k) else np.nan,
                'Pval_KINâ†’EMG': float(p_k2e) if np.isfinite(p_k2e) else np.nan,
                'Pval_EMGâ†’KIN': float(p_e2k) if np.isfinite(p_e2k) else np.nan,
                'Granger_KINâ†’EMG': float(min_p_k2e) if np.isfinite(min_p_k2e) else np.nan,
                'Granger_EMGâ†’KIN': float(min_p_e2k) if np.isfinite(min_p_e2k) else np.nan,
            })

# =========================
# Compile outputs + FDR (per GroupÃ—Phase)
# =========================
results_df = pd.DataFrame(results).round(6)
if not results_df.empty:
    results_df = results_df.sort_values(['Group','Phase','Muscle','KinematicVar']).reset_index(drop=True)
    results_df['q_G_KINâ†’EMG'] = np.nan
    results_df['q_G_EMGâ†’KIN'] = np.nan
    for (g, ph), idx in results_df.groupby(['Group', 'Phase']).groups.items():
        k2e = results_df.loc[idx, 'Granger_KINâ†’EMG'].to_numpy()
        e2k = results_df.loc[idx, 'Granger_EMGâ†’KIN'].to_numpy()
        for arr, outcol in [(k2e, 'q_G_KINâ†’EMG'), (e2k, 'q_G_EMGâ†’KIN')]:
            mask = np.isfinite(arr)
            if mask.sum() > 1:
                q = np.full_like(arr, np.nan, dtype=float)
                q[mask] = multipletests(arr[mask], method='fdr_bh')[1]
                results_df.loc[idx, outcol] = q

if not results_df.empty:
    scroll_table("ğŸ§  Summary: EMG â‡„ Kinematics (XCorr, HAC OLS, Granger + FDR)", results_df,
                 note="XCorr: lag>0 â‡’ EMG lags KIN; q_G_* are BH-FDR within GroupÃ—Phase.")
else:
    print("No results to display (insufficient data after filtering).")

# =========================
# Add IRF metrics (both directions; auto horizon)
# =========================
if not results_df.empty:
    results_df = add_irf_metrics(
        avg_df, results_df,
        alpha=0.05, use_fdr=True,
        directions='all',
        orth='generalized',
        horizon='auto_optimal',
        stride_ms=DEFAULT_STRIDE_MS,
        max_pairs=None
    )
    scroll_table("ğŸ§  Summary + IRF metrics (auto-optimal horizons)", results_df.round(6),
                 note=("XCorr: lag>0 â‡’ EMG lags KIN; q_G_* are BH-FDR within GroupÃ—Phase. "
                       "IRF_*_H_chosen_* show the horizon used (steps/ms)."))
else:
    print("No results to display after adding IRF metrics.")

# =========================
# Optional: minimal plotting helper for quick QA
# =========================
def plot_group_comparison_per_phase(
    merged_df,
    emg="RF",
    kin="Angolo_Flex-ext ginocchio",
    time_col=TIME_COL,
    group_col=GROUP_COL,
    phase_col=PHASE_COL,
    groups=None,
    phases=None,
    min_points=MIN_POINTS,
    smooth_window=None,
    save=False,
    save_dir="plots_timeseries"
):
    if phases is None:
        phases = sorted(pd.Series(merged_df[phase_col].dropna()).unique())
    if groups is None:
        default_order = ["NESSUNO", "1", "2", "3"]
        present = [g for g in default_order if g in set(merged_df[group_col].dropna().unique())]
        extras = [g for g in pd.Series(merged_df[group_col].dropna()).unique() if g not in present]
        groups = present + extras
    needed = {time_col, emg, kin, group_col, phase_col}
    if any(c not in merged_df.columns for c in needed):
        raise ValueError("Missing columns in merged_df")
    save_path = Path(save_dir)
    if save: save_path.mkdir(parents=True, exist_ok=True)
    for phase in phases:
        plt.figure(figsize=(10, 6)); any_plotted = False
        for g in groups:
            sub = merged_df.loc[(merged_df[group_col]==g) & (merged_df[phase_col]==phase), [time_col, emg, kin]].dropna()
            if len(sub) < min_points: continue
            sub = sub.sort_values(time_col)
            kin_series = _rolling_smooth(sub[kin].to_numpy(), smooth_window)
            emg_series = _rolling_smooth(sub[emg].to_numpy(), smooth_window)
            xz = _safe_z(kin_series); yz = _safe_z(emg_series)
            plt.plot(sub[time_col].to_numpy(), xz, linestyle="-",  alpha=0.9, label=f"{g} â€“ {kin}")
            plt.plot(sub[time_col].to_numpy(), yz, linestyle="--", alpha=0.9, label=f"{g} â€“ {emg}")
            any_plotted = True
        plt.title(f"{emg} and {kin} (z-score) across Groups â€“ Phase: {phase}")
        plt.xlabel("% Gait"); plt.ylabel("z-score")
        if any_plotted: plt.legend(loc="upper right", fontsize="small")
        plt.tight_layout()
        if save:
            safe_phase = str(phase).replace("/", "_").replace("\\", "_")
            plt.savefig(save_path / f"{emg}__{kin}__phase_{safe_phase}.png", dpi=150); plt.close()
        else:
            plt.show()


# Save table (use Excel to avoid locale parsing issues)
results_df.to_excel("results_irf.xlsx", index=False)
# Optionally also CSV for Italian Excel:
# results_df.to_csv("results_irf_IT.csv", sep=";", decimal=",", index=False, encoding="utf-8")

###==============================================================================================================================================================####
####           PLOTTING RESULTS          ########
# ================================
# Quadriceps Ã— Knee predictors â€” 3Ã—3 grids per group (global 0â€“100% gait)
# Z-score BOTH EMG and kinematics within phase; no phase labels; subtle separators.
# Colors: kinematics (solid blue) matches Kâ†’E band; EMG (dashed orange) matches Eâ†’K hatched band.
# Y-axis fixed across ALL plots/figures via y_lim (default: (-3, 3)).
# Requires: avg_df (merged per-phase EMG+KIN) and results_df (q & p values).
# ================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

# ---- Colors (lines and bands stay consistent) ----
K2E_BLUE   = "#2F6BFF"   # kinematicâ†’EMG (line + band)
E2K_ORANGE = "#E36C0A"   # EMGâ†’kinematic (line + hatched band)

# ---- Aesthetics ----
plt.rcParams.update({
    "font.size": 10,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "axes.titleweight": "semibold",
    "axes.grid": True,
    "grid.alpha": 0.18,
    "grid.linewidth": 0.6,
})

# ---- Labels / ordering ----
MUSCLE_ORDER = ["RF", "VM", "VL"]
MUSCLE_FULL  = {"RF": "rectus femoris", "VM": "vastus medialis", "VL": "vastus lateralis"}

KIN_LABELS   = {
    "Angolo_Flex-ext ginocchio": "knee angle",
    "Vel_Flex-ext ginocchio": "knee angular velocity",
    "Acc_Flex-ext ginocchio": "knee angular acceleration",
}
KIN_ORDER = ["Angolo_Flex-ext ginocchio", "Vel_Flex-ext ginocchio", "Acc_Flex-ext ginocchio"]

PHASE_ORDER = ["LoadingResponse", "MidStance", "TerminalStance",
               "PreSwing", "InitialSwing", "MidSwing", "TerminalSwing"]

# Default phase widths (sum ~100). Replace with your cohort-specific widths if available.
DEFAULT_PHASE_WIDTHS = {
    "LoadingResponse": 12, "MidStance": 14, "TerminalStance": 12, "PreSwing": 12,
    "InitialSwing": 12, "MidSwing": 14, "TerminalSwing": 24,
}

GROUP_FULL = {"NESSUNO": "controls", "3": "group 3 (mild HSP)",
              "2": "group 2 (moderate HSP)", "1": "group 1 (severe HSP)"}

# ---- Helpers ----
def _classify_sync(q, p):
    """Return 'sync' if q<0.05 & p<0.05, 'async' if q<0.05 & p>=0.05, else None."""
    if pd.isna(q) or pd.isna(p): return None
    if q < 0.05 and p < 0.05: return "sync"
    if q < 0.05 and p >= 0.05: return "async"
    return None

def _smooth(a, w):
    if not w or w <= 1: return np.asarray(a, float)
    s = pd.Series(a, dtype="float64")
    return s.rolling(window=w, min_periods=max(1, w//3), center=True).mean().to_numpy()

def _z(a):
    a = np.asarray(a, float)
    m, s = np.nanmean(a), np.nanstd(a)
    return (a - m)/s if (np.isfinite(s) and s > 0) else np.zeros_like(a)

def _phase_spans_global(phase_widths=None):
    """Map each phase to (start,end) in global % gait (0â€“100)."""
    widths = phase_widths or DEFAULT_PHASE_WIDTHS
    total = float(sum(widths.get(ph, 0) for ph in PHASE_ORDER))
    f = 100.0 / total if total > 0 else 1.0
    spans, x = {}, 0.0
    for ph in PHASE_ORDER:
        w = widths.get(ph, 0) * f
        spans[ph] = (x, x + w)
        x += w
    return spans

def _map_local_to_global(phase, local_pct, spans):
    """Map local 0â€“100% within phase to global % gait using spans dict."""
    x0, x1 = spans[phase]
    return x0 + (np.asarray(local_pct, float) / 100.0) * (x1 - x0)

# ---- Main plotter ----
def plot_grid_optionA_global(
    merged_df, results_df, *,
    group="1",
    smooth_window=5,
    phase_widths=None,
    out_path="outputs/optionA_grid.png",
    figsize=(13, 9),
    time_col="Percentage",
    group_col="SOTTOGRUPPO",
    phase_col="PHASE",
    show_phase_lines=True,
    y_lim=(-3, 3),       # <<< fixed y-axis for ALL plots/figures (z-scored signals)
):
    """
    Build a 3Ã—3 grid (rows=RF/VM/VL; cols=knee angle/velocity/acceleration),
    with global 0â€“100% gait x-axis. Z-score BOTH signals within phase.
    Per-phase driver shading from results_df. Phase names suppressed; optional separators.
    """
    Path(out_path).parent.mkdir(parents=True, exist_ok=True)
    spans = _phase_spans_global(phase_widths)

    fig, axes = plt.subplots(3, 3, figsize=figsize, sharex=True, sharey=True)

    # Optional vertical phase separators (no labels)
    if show_phase_lines:
        for ph in PHASE_ORDER:
            x0, _ = spans[ph]
            for ax in axes.ravel():
                ax.axvline(x0, color='k', lw=0.6, alpha=0.16, zorder=1)
        for ax in axes.ravel():  # final boundary at 100%
            ax.axvline(100, color='k', lw=0.6, alpha=0.16, zorder=1)

    for r, muscle in enumerate(MUSCLE_ORDER):
        for c, kin_col in enumerate(KIN_ORDER):
            ax = axes[r, c]
            X_glob, Y_emg, Y_kin = [], [], []

            for ph in PHASE_ORDER:
                sub = merged_df.loc[
                    (merged_df[group_col] == group) &
                    (merged_df[phase_col] == ph),
                    [time_col, muscle, kin_col]
                ].dropna()
                if sub.empty:
                    continue

                sub = sub.sort_values(time_col)
                x_glob = _map_local_to_global(ph, sub[time_col].to_numpy(), spans)

                emg_vals = _smooth(sub[muscle].to_numpy(), smooth_window)
                kin_vals = _smooth(sub[kin_col].to_numpy(), smooth_window)

                # z-score BOTH within phase
                emg_plot = _z(emg_vals)
                kin_plot = _z(kin_vals)

                X_glob.append(x_glob)
                Y_emg.append(emg_plot)
                Y_kin.append(kin_plot)

                # Per-phase driver shading (bands only; no text)
                row = results_df.loc[
                    (results_df['Group'] == group) &
                    (results_df['Phase'] == ph) &
                    (results_df['Muscle'] == muscle) &
                    (results_df['KinematicVar'] == kin_col)
                ]
                if not row.empty:
                    x0, x1 = spans[ph]
                    # Kâ†’E (solid blue)
                    st_k2e = _classify_sync(
                        float(row['q_G_KINâ†’EMG'].iloc[0]) if 'q_G_KINâ†’EMG' in row else np.nan,
                        float(row['Pval_KINâ†’EMG'].iloc[0]) if 'Pval_KINâ†’EMG' in row else np.nan
                    )
                    if st_k2e is not None:
                        ax.add_patch(plt.Rectangle(
                            (x0, y_lim[0]-1), x1 - x0, (y_lim[1]-y_lim[0])+2,  # tall band w.r.t. fixed y
                            facecolor=K2E_BLUE, alpha=0.12,
                            edgecolor=K2E_BLUE, linestyle=':' if st_k2e == 'async' else '-', linewidth=1.1,
                            zorder=0
                        ))
                    # Eâ†’K (orange hatched)
                    st_e2k = _classify_sync(
                        float(row['q_G_EMGâ†’KIN'].iloc[0]) if 'q_G_EMGâ†’KIN' in row else np.nan,
                        float(row['Pval_EMGâ†’KIN'].iloc[0]) if 'Pval_EMGâ†’KIN' in row else np.nan
                    )
                    if st_e2k is not None:
                        ax.add_patch(plt.Rectangle(
                            (x0, y_lim[0]-1), x1 - x0, (y_lim[1]-y_lim[0])+2,
                            fill=False, hatch='///', edgecolor=E2K_ORANGE,
                            linestyle=':' if st_e2k == 'async' else '-', linewidth=1.1,
                            zorder=2
                        ))

            # Draw traces (use SAME colors as bands)
            if X_glob:
                X  = np.concatenate(X_glob)
                Em = np.concatenate(Y_emg)
                K  = np.concatenate(Y_kin)
                ax.plot(X, K,  '-',  linewidth=2.0, color=K2E_BLUE,   label=KIN_LABELS[kin_col], zorder=3)
                ax.plot(X, Em, '--', linewidth=2.0, color=E2K_ORANGE, label=MUSCLE_FULL[muscle], zorder=3)

            # Titles/labels
            if r == 0:
                ax.set_title(KIN_LABELS[kin_col], fontsize=11, pad=6)
            if c == 0:
                ax.set_ylabel(MUSCLE_FULL[muscle], fontsize=10)

            # Axes cosmetics + fixed limits
            ax.set_xlim(0, 100)
            ax.set_xticks([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])
            ax.set_ylim(*y_lim)

    fig.suptitle(f"{GROUP_FULL.get(group, group)} â€” quadriceps vs. knee predictors (global 0â€“100% gait)", fontsize=14)
    for ax in axes[-1, :]:
        ax.set_xlabel("% gait (global)")

    plt.tight_layout(rect=[0.03, 0.03, 0.86, 0.94])
    plt.savefig(out_path, dpi=300)
    plt.close(fig)
    print(f"Saved: {out_path}")

# ---- Generate all four grids (shared fixed y-axis) ----
outdir = Path("outputs"); outdir.mkdir(exist_ok=True, parents=True)
for g in ["NESSUNO", "3", "2", "1"]:
    plot_grid_optionA_global(
        merged_df=avg_df,
        results_df=results_df,
        group=g,
        smooth_window=5,
        out_path=str(outdir / f"optionA_grid_{g}.png"),
        show_phase_lines=True,
        y_lim=(-3, 3)  # <-- fixed across ALL figures
    )


#########################################################################################################################
##    Heatmaps  ###
########################################################################################################################    
# ============================================================
# Controller presence "flow map" per group (no seaborn / no SciPy)
# - x: 0â€“100% gait (true phase widths)
# - y: soft bands for RF / VM / VL
# - Blue = Kâ†’E evidence (sync=1.0, async=0.5); Orange = Eâ†’K evidence (same)
# - White contour marks dominance boundary (Blueâ‰ˆOrange)
# Saves PNGs to outputs/controller_flowmap_<group>.png
# ============================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap, Normalize
from pathlib import Path

# ----------------- CONFIG -----------------
PHASE_ORDER  = ["LoadingResponse","MidStance","TerminalStance","PreSwing","InitialSwing","MidSwing","TerminalSwing"]
MUSCLE_ORDER = ["RF","VM","VL"]
GROUPS       = ["NESSUNO","3","2","1"]
GROUP_FULL   = {"NESSUNO":"Controls","3":"Group 3 (mild HSP)","2":"Group 2 (moderate HSP)","1":"Group 1 (severe HSP)"}
MUSCLE_FULL  = {"RF":"rectus femoris","VM":"vastus medialis","VL":"vastus lateralis"}

# phase widths (edit if you have cohort-specific)
DEFAULT_PHASE_WIDTHS = {
    "LoadingResponse": 12, "MidStance": 14, "TerminalStance": 12, "PreSwing": 12,
    "InitialSwing": 12, "MidSwing": 14, "TerminalSwing": 24,
}

# colors (match your other figs)
K2E_BLUE   = "#2F6BFF"
E2K_ORANGE = "#E36C0A"

# canvas resolution
NX = 1000   # horizontal pixels (0..100% gait)
NY = 300    # vertical pixels (stacked smooth bands)

# weighting (sync counts matter more than async)
W_SYNC  = 1.0
W_ASYNC = 0.5

# gaussian blur (in pixels). Increase for smoother blobs.
SIGMA_X = 20
SIGMA_Y = 18

# ----------------- HELPERS -----------------
def _classify(q, p):
    if pd.isna(q) or pd.isna(p): return None
    if q < 0.05 and p < 0.05: return "sync"
    if q < 0.05 and p >= 0.05: return "async"
    return None

def _phase_spans_global(widths=None):
    w = widths or DEFAULT_PHASE_WIDTHS
    total = float(sum(w.get(p,0) for p in PHASE_ORDER))
    f = 100.0 / total if total>0 else 1.0
    spans = {}; x=0.0
    for p in PHASE_ORDER:
        spans[p] = (x, x + w[p]*f); x = spans[p][1]
    return spans

def _build_counts(results_df):
    """Counts per GroupÃ—PhaseÃ—Muscle for K2E/E2K split by sync/async."""
    knee_vars = ["Angolo_Flex-ext ginocchio","Vel_Flex-ext ginocchio","Acc_Flex-ext ginocchio"]
    rows=[]
    for (g, ph, m), sub in results_df.groupby(["Group","Phase","Muscle"], observed=True):
        sub = sub[sub["KinematicVar"].isin(knee_vars)]
        k2e_sync=k2e_async=e2k_sync=e2k_async=0
        for _, r in sub.iterrows():
            s1 = _classify(r.get("q_G_KINâ†’EMG", np.nan), r.get("Pval_KINâ†’EMG", np.nan))
            if   s1=="sync":  k2e_sync += 1
            elif s1=="async": k2e_async += 1
            s2 = _classify(r.get("q_G_EMGâ†’KIN", np.nan), r.get("Pval_EMGâ†’KIN", np.nan))
            if   s2=="sync":  e2k_sync += 1
            elif s2=="async": e2k_async += 1
        rows.append({"Group":g,"Phase":ph,"Muscle":m,
                     "K2E_sync":k2e_sync,"K2E_async":k2e_async,
                     "E2K_sync":e2k_sync,"E2K_async":e2k_async})
    df = pd.DataFrame(rows)
    idx = pd.MultiIndex.from_product([GROUPS, PHASE_ORDER, MUSCLE_ORDER],
                                     names=["Group","Phase","Muscle"])
    df = df.set_index(["Group","Phase","Muscle"]).reindex(idx).fillna(0).reset_index()
    for c in ["K2E_sync","K2E_async","E2K_sync","E2K_async"]: df[c] = df[c].astype(int)
    return df

def _gaussian_kernel1d(sigma, radius=None):
    if sigma <= 0: return np.array([1.0])
    if radius is None:
        radius = int(3*sigma)  # ~99.7% mass
    x = np.arange(-radius, radius+1, dtype=float)
    k = np.exp(-(x**2) / (2*sigma*sigma))
    k /= k.sum()
    return k

def _blur2d(img, sx, sy):
    """Separable Gaussian blur with 1D kernels (NumPy-only)."""
    ky = _gaussian_kernel1d(sy)
    kx = _gaussian_kernel1d(sx)
    # pad and convolve along y
    pad_y = len(ky)//2
    tmp = np.pad(img, ((pad_y,pad_y),(0,0)), mode="edge")
    tmp = np.apply_along_axis(lambda v: np.convolve(v, ky, mode="valid"), 0, tmp)
    # pad and convolve along x
    pad_x = len(kx)//2
    tmp = np.pad(tmp, ((0,0),(pad_x,pad_x)), mode="edge")
    out = np.apply_along_axis(lambda v: np.convolve(v, kx, mode="valid"), 1, tmp)
    return out

def _lin_colormap(color_hex):
    """transparentâ†’color colormap."""
    rgb = plt.matplotlib.colors.to_rgb(color_hex)
    return LinearSegmentedColormap.from_list("lin", [(rgb[0],rgb[1],rgb[2],0.0), (*rgb,1.0)])

# ----------------- CORE PLOT -----------------
def plot_controller_flowmap(results_df, out_dir="outputs"):
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    counts = _build_counts(results_df)
    spans  = _phase_spans_global()

    # normalize phase spans to pixel columns
    edges = [spans[PHASE_ORDER[0]][0]] + [spans[p][1] for p in PHASE_ORDER]
    x_edges = np.array(edges) * (NX/100.0)
    x_edges = np.rint(x_edges).astype(int)
    x_edges[-1] = NX

    # vertical centers for RF/VM/VL
    yc = np.linspace(NY*0.2, NY*0.8, 3)  # spread nicely
    band_h = NY * 0.12                   # band half-height

    # per group figure
    for g in GROUPS:
        Zk = np.zeros((NY, NX), dtype=float)  # Kâ†’E evidence
        Ze = np.zeros((NY, NX), dtype=float)  # Eâ†’K evidence

        subg = counts[counts["Group"]==g]
        for mi, m in enumerate(MUSCLE_ORDER):
            subm = subg[subg["Muscle"]==m].set_index("Phase").reindex(PHASE_ORDER)

            # per-phase â€œevidenceâ€ (sync*1 + async*0.5), in [0..3*1 + 3*0.5] = [0..4.5]
            ev_k = (W_SYNC*subm["K2E_sync"] + W_ASYNC*subm["K2E_async"]).to_numpy(dtype=float)
            ev_e = (W_SYNC*subm["E2K_sync"] + W_ASYNC*subm["E2K_async"]).to_numpy(dtype=float)

            # fill horizontal strips phase-by-phase
            for pi in range(len(PHASE_ORDER)):
                x0, x1 = x_edges[pi], x_edges[pi+1]
                # vertical weighting (soft band around muscle center)
                y = np.arange(NY)
                vy = np.exp(-0.5 * ((y - yc[mi]) / band_h)**2)
                # normalize vy to [0..1]
                vy /= vy.max() if vy.max()>0 else 1.0
                Zk[:, x0:x1] += vy[:, None] * ev_k[pi]
                Ze[:, x0:x1] += vy[:, None] * ev_e[pi]

        # blur to get â€œblobâ€ look
        Zk_b = _blur2d(Zk, SIGMA_X, SIGMA_Y)
        Ze_b = _blur2d(Ze, SIGMA_X, SIGMA_Y)

        # scale each to [0..1] by its own max (or jointlyâ€”pick one)
        mmax = max(Zk_b.max(), Ze_b.max(), 1e-6)
        Zk_n = Zk_b / mmax
        Ze_n = Ze_b / mmax

        # dominance boundary: where Zkâ‰ˆZe
        D = Zk_n - Ze_n

        # draw
        fig, ax = plt.subplots(figsize=(12, 4.2))
        ax.set_facecolor("#F8F8F8")

        im1 = ax.imshow(Zk_n, cmap=_lin_colormap(K2E_BLUE), origin="lower",
                        extent=[0,100,0,100], interpolation="bicubic")
        im2 = ax.imshow(Ze_n, cmap=_lin_colormap(E2K_ORANGE), origin="lower",
                        extent=[0,100,0,100], interpolation="bicubic")

        # phase separators
        for p in PHASE_ORDER:
            ax.axvline(spans[p][0], color="#1C63FF", lw=0.8, ls="--", alpha=0.35)
        ax.axvline(100, color="#1C63FF", lw=0.8, ls="--", alpha=0.35)

        # muscle labels on the left margin at band centers
        for mi, m in enumerate(MUSCLE_ORDER):
            ax.text(-2, (yc[mi]/NY)*100, MUSCLE_FULL[m], ha="right", va="center", fontsize=10)

        ax.set_xlim(0,100); ax.set_ylim(0,100)
        ax.set_xlabel("% gait (global)")
        ax.set_yticks([])  # y is just band space
        ax.set_title(f"{GROUP_FULL.get(g,g)} â€” controller presence (blue: Kâ†’E, orange: Eâ†’K)")

        # small color scales (optional):
        from matplotlib.colorbar import ColorbarBase
        from mpl_toolkits.axes_grid1.inset_locator import inset_axes
        axins1 = inset_axes(ax, width="25%", height="5%", loc="upper left", borderpad=1.2)
        ColorbarBase(axins1, cmap=_lin_colormap(K2E_BLUE), norm=Normalize(0,1), orientation="horizontal")
        axins1.set_title("Kâ†’E evidence", fontsize=9, pad=2)
        axins1.set_xticks([0,0.5,1]); axins1.set_xticklabels(["low","mid","high"])

        axins2 = inset_axes(ax, width="25%", height="5%", loc="upper right", borderpad=1.2)
        ColorbarBase(axins2, cmap=_lin_colormap(E2K_ORANGE), norm=Normalize(0,1), orientation="horizontal")
        axins2.set_title("Eâ†’K evidence", fontsize=9, pad=2)
        axins2.set_xticks([0,0.5,1]); axins2.set_xticklabels(["low","mid","high"])

        Path(out_dir).mkdir(parents=True, exist_ok=True)
        out = Path(out_dir)/f"controller_flowmap_{g}.png"
        plt.tight_layout()
        plt.savefig(out, dpi=300)
        plt.close(fig)
        print("Saved:", out)

# ---------- run ----------
plot_controller_flowmap(results_df, out_dir="outputs")





        
##################################################################################################       
# ============================================================
# Colored preprocessed EMG grids (uses group_signals EXACTLY)
# - one figure per group; rows = RF/VM/VL, cols = phases (8 or 7)
# - mean Â± SD from group_signals; mean curve colored by phase dominance
# - dominance from results_df for knee vars, per phase & muscle
# - saves to outputs/emg_preprocessed_colored_grid_<group>.png
# ============================================================
# Single-trace (0â€“100%) per muscle with subphase coloring
# - uses preprocessed envelopes from `group_signals` (raw normalized units)
# - colors only the subphase segment on the single trace
# - one figure per group, 3 rows (RF/VM/VL)
# - saves to outputs/emg_preprocessed_whole_colored_<group>.png
# ============================================================
####################################################################################################
##================= Grid of 3Ã—3 EMG-Kin Plots per Group ==========================================##
####################################################################################################
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib import cm
from pathlib import Path

# -------------------------
# Constants
# -------------------------

PHASES_8 = ['IC', 'LR', 'MSt', 'TSt', 'PSw', 'ISw', 'MSw', 'TSw']
MUSCLES = ['RF', 'VM', 'VL']
MUSCLE_FULL = {'RF': 'rectus femoris', 'VM': 'vastus medialis', 'VL': 'vastus lateralis'}
KIN_VARS = ["Angolo_Flex-ext ginocchio", "Vel_Flex-ext ginocchio", "Acc_Flex-ext ginocchio"]
KIN_SHORT = {'Angolo_Flex-ext ginocchio': 'Angle',
             'Vel_Flex-ext ginocchio': 'Velocity',
             'Acc_Flex-ext ginocchio': 'Acceleration'}
PHASE_WIDTHS = {'IC':12, 'LR':14, 'MSt':12, 'TSt':12, 'PSw':12, 'ISw':12, 'MSw':14, 'TSw':24}

# -------------------------
# Helpers
# -------------------------

def norm_group(g):
    s = str(g).strip().upper()
    return {'NORMATIVI': 'NESSUNO', 'GRUPPO1': '1', 'GRUPPO2': '2', 'GRUPPO3': '3'}.get(s, s)

def norm_phase(p):
    LONG2SHORT = {
        'LoadingResponse': 'IC', 'MidStance': 'MSt', 'TerminalStance': 'TSt',
        'PreSwing': 'PSw', 'InitialSwing': 'ISw', 'MidSwing': 'MSw', 'TerminalSwing': 'TSw'
    }
    p = str(p).strip()
    return p if p in PHASES_8 else LONG2SHORT.get(p, p)

def phase_spans(widths=None):
    if widths is None:
        w = {ph: 100 / len(PHASES_8) for ph in PHASES_8}
    else:
        total = float(sum(widths[p] for p in PHASES_8))
        w = {p: 100.0 * widths[p] / total for p in PHASES_8}
    spans, x = {}, 0.0
    for ph in PHASES_8:
        spans[ph] = (x, x + w[ph])
        x += w[ph]
    return spans

# -------------------------
# Build score map: directional dominance by (group, phase, muscle, kinematic variable)
# -------------------------

def build_score_map_per_kin(results_df):
    R = results_df.copy()
    R["Group_n"] = R["Group"].map(norm_group)
    R["Phase_n"] = R["Phase"].map(norm_phase)

    score = {}
    for (g, ph, m, kin), sub in R.groupby(["Group_n", "Phase_n", "Muscle", "KinematicVar"], observed=True):
        k2e_sync  = ((sub["q_G_KINâ†’EMG"] < 0.05) & (sub["Pval_KINâ†’EMG"] < 0.05)).fillna(False).sum()
        k2e_async = ((sub["q_G_KINâ†’EMG"] < 0.05) & (sub["Pval_KINâ†’EMG"] >= 0.05)).fillna(False).sum()
        e2k_sync  = ((sub["q_G_EMGâ†’KIN"] < 0.05) & (sub["Pval_EMGâ†’KIN"] < 0.05)).fillna(False).sum()
        e2k_async = ((sub["q_G_EMGâ†’KIN"] < 0.05) & (sub["Pval_EMGâ†’KIN"] >= 0.05)).fillna(False).sum()
        val = (k2e_sync + 0.5 * k2e_async) - (e2k_sync + 0.5 * e2k_async)
        score[(g, ph, m, kin)] = float(np.clip(val / 4.5, -1.0, 1.0))

    # Fill missing entries
    groups = R["Group_n"].unique()
    for g in groups:
        for ph in PHASES_8:
            for m in MUSCLES:
                for kin in KIN_VARS:
                    score.setdefault((g, ph, m, kin), 0.0)

    return score

# -------------------------
# Main Plotting Function
# -------------------------

def plot_grid_emg_kin(group, scores, avg_df, out_dir="outputs"):
    g = norm_group(group)
    spans = phase_spans(PHASE_WIDTHS)

    fig, axes = plt.subplots(3, 3, figsize=(14, 8), sharex=True, sharey=True)
    fig.patch.set_facecolor("white")

    # ğŸŒˆ CHANGE COLORMAP HERE (e.g. PiYG, RdBu, PRGn, etc.)
    DIR_CMAP = cm.get_cmap('PiYG')
    DIR_NORM = Normalize(-1, 1)

    for i_m, m in enumerate(MUSCLES):
        for i_k, kin_var in enumerate(KIN_VARS):
            ax = axes[i_m, i_k]

            # Subset
            sub_df = avg_df[
                (avg_df['SOTTOGRUPPO'] == g) &
                avg_df[m].notna() &
                avg_df[kin_var].notna()
            ].copy()

            # Bin
            bins = np.arange(0, 101)
            sub_df['GaitBin'] = pd.cut(sub_df['Percentage'], bins=bins, labels=bins[:-1], include_lowest=True)
            binned = sub_df.groupby('GaitBin')[[m, kin_var]].agg(['mean', 'std']).reset_index()
            binned.columns = ['GaitBin', 'emg_mean', 'emg_std', 'kin_mean', 'kin_std']
            binned['GaitBin'] = binned['GaitBin'].astype(int)

            # Ensure 0â€“99 bins
            full_bins = pd.DataFrame({'GaitBin': np.arange(100)})
            binned = full_bins.merge(binned, on='GaitBin', how='left')

            # Interpolate and fill
            for col in ['emg_mean', 'emg_std', 'kin_mean', 'kin_std']:
                binned[col] = binned[col].interpolate(limit_direction='both').fillna(0)

            x = binned['GaitBin'].to_numpy()
            y_emg = binned['emg_mean'].to_numpy()
            y_std = binned['emg_std'].to_numpy()

            y_kin = binned['kin_mean'].to_numpy()

            # âœ… Robust 1â€“99 percentile normalization
            q1, q99 = np.nanpercentile(y_kin, 1), np.nanpercentile(y_kin, 99)
            if q99 - q1 < 1e-6:
                y_kin_norm = np.zeros_like(y_kin)
            else:
                y_kin_norm = (y_kin - q1) / (q99 - q1)
                y_kin_norm = np.clip(y_kin_norm, 0, 1)

            # Directional dominance score per phase
            color_vals = np.zeros_like(x, dtype=float)
            for ph in PHASES_8:
                x0, x1 = spans[ph]
                mask = (x >= x0) & (x <= x1)
                color_vals[mask] = scores.get((g, ph, m, kin_var), 0.0)

            # Fill under EMG trace
            for i in range(len(x) - 1):
                x0, x1 = x[i], x[i + 1]
                y0, y1 = y_emg[i], y_emg[i + 1]
                ax.fill([x0, x1, x1, x0], [0, 0, y1, y0],
                        color=DIR_CMAP(DIR_NORM(color_vals[i])),
                        zorder=1)

            # EMG line
            ax.plot(x, y_emg, color='black', lw=1.2, zorder=3)

            # Kinematic line (normalized)
            ax.plot(x, y_kin_norm, '--', color='gray', lw=1.2, zorder=4)

            # Formatting
            ax.set_xlim(0, 100)
            ax.set_ylim(0, 1)
            ax.set_xticks(np.arange(0, 101, 10))
            ax.grid(True, alpha=0.1)
            ax.spines[['top', 'right']].set_visible(False)

            # Labels
            if i_k == 0:
                ax.set_ylabel(f"{MUSCLE_FULL[m]}")
            if i_m == 2:
                ax.set_xlabel("% Gait")
            if i_m == 0:
                ax.set_title(KIN_SHORT[kin_var], fontsize=11)

    # Colorbar
    cax = fig.add_axes([0.91, 0.15, 0.015, 0.7])
    cb = plt.colorbar(cm.ScalarMappable(norm=DIR_NORM, cmap=DIR_CMAP), cax=cax)
    cb.set_ticks([-1, 0, 1])
    cb.set_ticklabels(["Eâ†’K", "tie", "Kâ†’E"])
    cb.set_label("Directional dominance")
    cb.outline.set_visible(False)

    # Title
    title_map = {
        'NESSUNO': 'Controls',
        '1': 'Group 1 (severe HSP)',
        '2': 'Group 2 (moderate HSP)',
        '3': 'Group 3 (mild HSP)'
    }
    fig.suptitle(title_map.get(g, g), fontsize=14)

    # Save
    fig.tight_layout(rect=[0.04, 0.05, 0.9, 0.94])
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    out = Path(out_dir) / f"emg_kin_grid_{g}.png"
    fig.savefig(out, dpi=300, facecolor=fig.get_facecolor())
    plt.close(fig)
    print("âœ… Saved:", out)


# -------------------------
# Run the plotting
# -------------------------

score_map = build_score_map_per_kin(results_df)

for g in avg_df['SOTTOGRUPPO'].dropna().unique():
    plot_grid_emg_kin(g, score_map, avg_df, out_dir="outputs")


#####################################################################################################
####################################################################################################
##================= Grid of 3Ã—2 EMG-KIN Plots per Group â€“ Accurate Directional Rules ==============##
####################################################################################################
# =====================================
# FINAL PLOTTING SECTION (standalone)
# =====================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
from matplotlib import cm
from pathlib import Path
from matplotlib.patches import Polygon


def norm_group(g):
    s = str(g).strip().upper()
    return {'NORMATIVI': 'NESSUNO', 'GRUPPO1': '1', 'GRUPPO2': '2', 'GRUPPO3': '3'}.get(s, s)

def norm_phase(p):
    LONG2SHORT = {
        'LoadingResponse': 'IC', 'MidStance': 'MSt', 'TerminalStance': 'TSt',
        'PreSwing': 'PSw', 'InitialSwing': 'ISw', 'MidSwing': 'MSw', 'TerminalSwing': 'TSw'
    }
    p = str(p).strip()
    return p if p in PHASES_8 else LONG2SHORT.get(p, p)

def phase_spans(widths=None):
    if widths is None:
        w = {ph: 100 / len(PHASES_8) for ph in PHASES_8}
    else:
        total = float(sum(widths[p] for p in PHASES_8))
        w = {p: 100.0 * widths[p] / total for p in PHASES_8}
    spans, x = {}, 0.0
    for ph in PHASES_8:
        spans[ph] = (x, x + w[ph])
        x += w[ph]
    return spans

# -------------------------
# Build effect map with logic: directionality + Î²*
# -------------------------
def build_directional_effect_map(results_df):
    R = results_df.copy()
    R["Group_n"] = R["Group"].map(norm_group)
    R["Phase_n"] = R["Phase"].map(norm_phase)

    effect_map = {}
    sync_map = {}

    for (g, ph, m, kin), sub in R.groupby(["Group_n", "Phase_n", "Muscle", "KinematicVar"], observed=True):
        q_k2e = sub["q_G_KINâ†’EMG"].values[0]
        p_k2e = sub["Pval_KINâ†’EMG"].values[0]
        beta_k2e = sub["CoefStd_KINâ†’EMG"].values[0]

        q_e2k = sub["q_G_EMGâ†’KIN"].values[0]
        p_e2k = sub["Pval_EMGâ†’KIN"].values[0]
        beta_e2k = sub["CoefStd_EMGâ†’KIN"].values[0]

        sig_k2e = pd.notna(q_k2e) and q_k2e < 0.05
        sig_e2k = pd.notna(q_e2k) and q_e2k < 0.05

        is_sync = False
        value = 0.0

        # Both directions significant (sync), and p-values confirm
        if sig_k2e and sig_e2k and pd.notna(p_k2e) and p_k2e < 0.05 and pd.notna(p_e2k) and p_e2k < 0.05:
            is_sync = True
            if abs(beta_k2e) >= abs(beta_e2k):
                value = +abs(beta_k2e)
            else:
                value = -abs(beta_e2k)
        elif sig_k2e:
            value = +abs(beta_k2e)
        elif sig_e2k:
            value = -abs(beta_e2k)
        else:
            value = 0.0

        effect_map[(g, ph, m, kin)] = float(np.clip(value, -1, 1))
        sync_map[(g, ph, m, kin)] = is_sync

    return effect_map, sync_map


# -------------------------
# Plotting Function
# -------------------------

def plot_grid_emg_kin(group, effect_map, sync_map, avg_df, out_dir="outputs"):
    g = norm_group(group)
    spans = phase_spans(PHASE_WIDTHS)
    
    n_rows = len(MUSCLES)          # 3
    n_cols = len(KIN_VARS)         # 2 (can vary if KIN_VARS is modified)

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 2.5 * n_rows), sharex=True, sharey=True)
    axes = np.atleast_2d(axes)     # Ensure it's 2D

    fig.patch.set_facecolor("white")

    DIR_CMAP = cm.get_cmap('seismic')
    DIR_NORM = Normalize(-1, 1)

    for i_m, m in enumerate(MUSCLES):
        for i_k, kin_var in enumerate(KIN_VARS):
            ax = axes[i_m, i_k]

            sub_df = avg_df[(avg_df['SOTTOGRUPPO'] == g) & avg_df[m].notna() & avg_df[kin_var].notna()].copy()

            bins = np.arange(0, 101)
            sub_df['GaitBin'] = pd.cut(sub_df['Percentage'], bins=bins, labels=bins[:-1], include_lowest=True)
            binned = sub_df.groupby('GaitBin')[[m, kin_var]].agg(['mean', 'std']).reset_index()
            binned.columns = ['GaitBin', 'emg_mean', 'emg_std', 'kin_mean', 'kin_std']
            binned['GaitBin'] = binned['GaitBin'].astype(int)

            # Fill missing
            full_bins = pd.DataFrame({'GaitBin': np.arange(100)})
            binned = full_bins.merge(binned, on='GaitBin', how='left')
            for col in ['emg_mean', 'emg_std', 'kin_mean', 'kin_std']:
                binned[col] = binned[col].interpolate(limit_direction='both').fillna(0)

            x = binned['GaitBin'].to_numpy()
            y_emg = binned['emg_mean'].to_numpy()
            y_kin = binned['kin_mean'].to_numpy()

            y_kin_norm = (y_kin - np.nanmin(y_kin)) / (np.nanmax(y_kin) - np.nanmin(y_kin) + 1e-6)

            color_vals = np.zeros_like(x, dtype=float)
            sync_vals = np.zeros_like(x, dtype=bool)

            for ph in PHASES_8:
                x0, x1 = spans[ph]
                b0 = int(round(x0))
                b1 = int(round(x1))
                mask = (x >= b0) & (x < b1)
                val = effect_map.get((g, ph, m, kin_var), 0.0)
                sync = sync_map.get((g, ph, m, kin_var), False)
                color_vals[mask] = val
                sync_vals[mask] = sync

            for i in range(len(x) - 1):
                auc_x = [x[i], x[i + 1], x[i + 1], x[i]]
                auc_y = [0, 0, y_emg[i + 1], y_emg[i]]
                color = DIR_CMAP(DIR_NORM(color_vals[i]))

                # Base color
                poly = Polygon(xy=list(zip(auc_x, auc_y)), closed=True,
                               facecolor=color, edgecolor='none', zorder=2)
                ax.add_patch(poly)

                # Optional hatch
                if sync_vals[i]:
                    ax.fill_between(x=[x[i], x[i + 1]], y1=0, y2=[y_emg[i], y_emg[i + 1]],
                                    hatch='////', edgecolor='gray', facecolor='none',
                                    linewidth=0.0, zorder=3)

            # Plot lines
            ax.plot(x, y_emg, color='black', lw=1.2, zorder=4)
            ax.plot(x, y_kin_norm, '--', color='gray', lw=1.2, zorder=5)

            # Axes
            ax.set_xlim(0, 100)
            ax.set_ylim(0, 1)
            ax.set_xticks(np.arange(0, 101, 10))
            ax.grid(True, alpha=0.1)
            ax.spines[['top', 'right']].set_visible(False)

            if i_k == 0:
                ax.set_ylabel(f"{MUSCLE_FULL[m]}")
            if i_m == 2:
                ax.set_xlabel("% Gait")
            if i_m == 0:
                ax.set_title(KIN_SHORT[kin_var], fontsize=11)

    # Colorbar
    cax = fig.add_axes([0.92, 0.15, 0.015, 0.7])
    cb = plt.colorbar(cm.ScalarMappable(norm=DIR_NORM, cmap=DIR_CMAP), cax=cax)
    cb.set_ticks([-1, 0, 1])
    cb.set_ticklabels(["EMGâ†’KIN", "none", "KINâ†’EMG"])
    cb.outline.set_visible(False)
    cb.set_label("Directionality (Granger q + Î²*)")

    title_map = {
        'NESSUNO': 'Controls',
        '1': 'Group 1 (severe HSP)',
        '2': 'Group 2 (moderate HSP)',
        '3': 'Group 3 (mild HSP)'
    }
    fig.suptitle(title_map.get(g, g), fontsize=14)

    fig.tight_layout(rect=[0.04, 0.05, 0.9, 0.94])
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    out = Path(out_dir) / f"emg_kin_grid_beta_sync_{g}.png"
    fig.savefig(out, dpi=300, facecolor=fig.get_facecolor())
    plt.close(fig)
    print("âœ… Saved:", out)

# -------------------------
# Run
# -------------------------

effect_map, sync_map = build_directional_effect_map(results_df)

for g in avg_df['SOTTOGRUPPO'].dropna().unique():
    plot_grid_emg_kin(g, effect_map, sync_map, avg_df, out_dir="outputs")
    
    
############################################################################
### resume table
def build_summary_table_full(effect_map, sync_map, results_df):
    R = results_df.copy()
    R["Group_n"] = R["Group"].map(norm_group)
    R["Phase_n"] = R["Phase"].map(norm_phase)

    summary_rows = []

    for (g, ph, m, kin), val in effect_map.items():
        if val == 0.0:
            continue  # Skip non-significant (non-colored) entries

        direction = "KINâ†’EMG" if val > 0 else "EMGâ†’KIN"
        strength = abs(val)
        sync = sync_map.get((g, ph, m, kin), False)

        # Fetch data from original DataFrame
        row = R[(R["Group_n"] == g) &
                (R["Phase_n"] == ph) &
                (R["Muscle"] == m) &
                (R["KinematicVar"] == kin)]

        if row.empty:
            q_val = np.nan
            p_k2e = np.nan
            p_e2k = np.nan
        else:
            if direction == "KINâ†’EMG":
                q_val = row["q_G_KINâ†’EMG"].values[0]
            else:
                q_val = row["q_G_EMGâ†’KIN"].values[0]

            # Include p-values for synchronous only
            p_k2e = row["Pval_KINâ†’EMG"].values[0] if sync else np.nan
            p_e2k = row["Pval_EMGâ†’KIN"].values[0] if sync else np.nan

        summary_rows.append({
            "Group": g,
            "Phase": ph,
            "Muscle": m,
            "Kinematic Variable": kin,
            "Direction": direction,
            "EffectStrength (|Î²*|)": round(strength, 4),
            "Granger_q": round(q_val, 5) if pd.notna(q_val) else np.nan,
            "Synchronous": sync,
            "Pval_KINâ†’EMG (HAC)": round(p_k2e, 5) if pd.notna(p_k2e) else np.nan,
            "Pval_EMGâ†’KIN (HAC)": round(p_e2k, 5) if pd.notna(p_e2k) else np.nan
        })

    df = pd.DataFrame(summary_rows)
    df = df.sort_values(by=["Group", "Phase", "Muscle", "Kinematic Variable"])
    return df

summary_df = build_summary_table_full(effect_map, sync_map, results_df)

# Save to Excel
output_path = Path("outputs") / "summary_table_directionality_with_pvals.xlsx"
summary_df.to_excel(output_path, index=False)

print(f"âœ… Summary table saved to: {output_path}")


# ==================================================================================================================
# SECTION 4 -- Proximal (RF, VM, VL)  vs  Distal (GM, GL) â€” EMGâ†”EMG (XCorr, HAC-OLS, fast Granger + FDR + optional IRF)
# Question 3: Why are proximal muscles more informative than distal ones for predicting pathology/disability, per phase?
# ==================================================================================================================


import os
import numpy as np
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.multitest import multipletests
from statsmodels.tsa.api import VAR
from joblib import Parallel, delayed
import matplotlib.pyplot as plt

# ---- Config ----
GROUP_COL = "Group"
PHASE_COL = "Phase"
TIME_COL  = "Percentage"

# Canonical 7-phase order (used for normalization & plotting)
PHASE_ORDER = [
    "LoadingResponse",
    "MidStance",
    "TerminalStance",
    "PreSwing",
    "InitialSwing",
    "MidSwing",
    "TerminalSwing",
]

# If your data use numeric phase codes, map them here â†’ canonical names.
# Edit only if you know a different mapping in your dataset.
PHASE_CODE_TO_NAME = {
    1: "LoadingResponse",
    2: "MidStance",
    3: "TerminalStance",
    4: "PreSwing",
    5: "InitialSwing",
    6: "MidSwing",
    7: "TerminalSwing",
    # 8: (intentionally unmapped)
}

# Data/compute knobs
MIN_POINTS         = 30           # minimum aligned points per pair
FAST_MODE          = True
DEFAULT_STRIDE_MS  = 1000
RNG_SEED           = 13

# Speed knobs (fast, robust defaults)
GC_P               = 2            # fixed VAR order for Granger p-values (use 1 if phases are very short)
XCORR_MAXLAG       = 10           # +/- lag window for cross-correlation
HAC_MAXLAG_CAP     = 10           # cap HAC bandwidth
N_JOBS             = -1           # parallel jobs across pairs

# If your raw CSV uses different names for GM/GL, map them here (raw_name -> 'GM'/'GL')
DISTAL_COLMAP = {}

rng = np.random.default_rng(RNG_SEED)

# ---------------- Phase normalization ----------------
def normalize_phase_names(df: pd.DataFrame, col: str = PHASE_COL) -> pd.DataFrame:
    """
    Coerce the Phase column to the 7 canonical names and set an ordered category.
    Numeric-like values are mapped via PHASE_CODE_TO_NAME; unmatched â†’ 'UnknownPhase' (then dropped in visuals).
    """
    if df is None or df.empty or col not in df.columns:
        return df
    d = df.copy()

    def _to_name(v):
        # numeric-like?
        try:
            k = int(float(str(v)))
            if k in PHASE_CODE_TO_NAME:
                return PHASE_CODE_TO_NAME[k]
        except Exception:
            pass
        s = str(v).strip()
        if s in PHASE_ORDER:
            return s
        return "UnknownPhase"

    d[col] = d[col].map(_to_name)
    d[col] = pd.Categorical(d[col], categories=PHASE_ORDER, ordered=True)
    unk = (d[col] == "UnknownPhase").sum()
    if unk > 0:
        print(f"âš ï¸ Phase normalization: {unk} rows labeled as 'UnknownPhase'. They will be excluded from plots.")
    return d

# --------------- small utils ---------------
def rms(x):
    x = np.asarray(x, dtype=float)
    return np.sqrt(np.nanmean(x**2)) if np.isfinite(x).any() else np.nan

def hac_maxlags_fast(n):
    # light HAC bandwidth that scales with n, capped for speed
    return max(1, min(HAC_MAXLAG_CAP, n // 8))

def xcorr_bounded(y, x, max_lag):
    """
    lag > 0 => y lags x (x leads). Returns (lags, corr).
    """
    y = np.asarray(y, dtype=float)
    x = np.asarray(x, dtype=float)
    L = min(len(y), len(x))
    if L < 3:
        lags = np.arange(-max_lag, max_lag + 1)
        return lags, np.full_like(lags, np.nan, dtype=float)

    y = y[:L] - np.nanmean(y[:L])
    x = x[:L] - np.nanmean(x[:L])
    sy, sx = np.nanstd(y), np.nanstd(x)
    if sy == 0 or sx == 0:
        lags = np.arange(-max_lag, max_lag + 1)
        return lags, np.full_like(lags, np.nan, dtype=float)

    lags = np.arange(-max_lag, max_lag + 1)
    cc = []
    for k in lags:
        if k >= 0:
            a, b = y[k:], x[:L - k]
        else:
            a, b = y[:L + k], x[-k:]
        if len(a) < 3: 
            cc.append(np.nan)
        else:
            cc.append(np.corrcoef(a, b)[0, 1])
    return lags, np.asarray(cc, dtype=float)

def zscore_fast(v):
    v = np.asarray(v, dtype=float)
    mu, sd = np.nanmean(v), np.nanstd(v)
    if not np.isfinite(sd) or sd == 0:
        return None
    return (v - mu) / sd

def granger_pvalue_fixed(y, x, p=GC_P):
    """
    Fast Granger: test x -> y using VAR([y,x]) at fixed order p.
    Returns a single p-value (F-test).
    """
    df = pd.DataFrame({"y": y, "x": x}).dropna()
    if len(df) <= p + 5:
        return np.nan
    try:
        res = VAR(df[["y", "x"]]).fit(p, trend='n')
        out = res.test_causality(caused="y", causing=["x"], kind='f')
        return float(out.pvalue)
    except Exception:
        return np.nan

# ---------- ensure GM/GL exist (augment from raw if needed) ----------
def ensure_distal_emg(avg_emg: pd.DataFrame, df_raw: pd.DataFrame, require_cols=("GM","GL")) -> pd.DataFrame:
    df = avg_emg.copy()
    need = [c for c in require_cols if c not in df.columns]
    if not need:
        return df

    if df_raw is None or df_raw.empty:
        print(f"âš ï¸ Distal EMG missing and no raw provided. Filling NaNs: {need}")
        for c in need: df[c] = np.nan
        return df

    raw = df_raw.copy()
    # optional renaming from raw -> canonical names
    for raw_name, std_name in DISTAL_COLMAP.items():
        if raw_name in raw.columns and std_name not in raw.columns:
            raw[std_name] = pd.to_numeric(raw[raw_name], errors='coerce')

    missing_in_raw = [c for c in need if c not in raw.columns]
    if missing_in_raw:
        print(f"âš ï¸ Distal EMG columns not found in raw CSV: {missing_in_raw}. Filling NaNs.")
        for c in missing_in_raw: df[c] = np.nan
        return df

    raw[TIME_COL] = pd.to_numeric(raw[TIME_COL], errors='coerce')
    raw[TIME_COL] = np.round(raw[TIME_COL]).astype("Int64")

    agg = (raw.groupby([GROUP_COL, PHASE_COL, TIME_COL])[list(need)].agg(rms).reset_index())
    df = df.merge(agg, on=[GROUP_COL, PHASE_COL, TIME_COL], how='left', suffixes=('', '_from_raw'))

    for c in need:
        cr = f"{c}_from_raw"
        if (c not in df.columns) or df[c].isna().all():
            df[c] = df[cr]
        if cr in df.columns:
            df.drop(columns=[cr], inplace=True)

    return df

# ---------- per-pair fast analysis ----------
def _analyze_pair(phase_df: pd.DataFrame, time_col: str, prox: str, dist: str) -> dict | None:
    cols = [time_col, prox, dist]
    if any(c not in phase_df.columns for c in cols):
        return None

    sub = phase_df[cols].dropna().sort_values(time_col)
    if len(sub) < MIN_POINTS:
        return None

    P_raw = sub[prox].to_numpy(float)
    D_raw = sub[dist].to_numpy(float)
    n_obs = len(sub)

    # Early flat check
    Pz = zscore_fast(P_raw)
    Dz = zscore_fast(D_raw)
    if Pz is None or Dz is None:
        return None

    # -------- Cross-corr on raw (interpretability) --------
    lDP, cDP = xcorr_bounded(D_raw, P_raw, max_lag=XCORR_MAXLAG)  # D lags P if lag>0
    lPD, cPD = xcorr_bounded(P_raw, D_raw, max_lag=XCORR_MAXLAG)  # P lags D if lag>0
    lag_DP = int(lDP[np.nanargmax(np.abs(cDP))]) if np.isfinite(cDP).any() else np.nan
    lag_PD = int(lPD[np.nanargmax(np.abs(cPD))]) if np.isfinite(cPD).any() else np.nan
    pk_DP  = float(np.nanmax(np.abs(cDP))) if np.isfinite(cDP).any() else np.nan
    pk_PD  = float(np.nanmax(np.abs(cPD))) if np.isfinite(cPD).any() else np.nan

    # -------- HAC-OLS (unstandardized p & R2 on raw) --------
    def hac_ols(y, x):
        X = sm.add_constant(x)
        res = sm.OLS(y, X).fit(cov_type="HAC", cov_kwds={"maxlags": hac_maxlags_fast(len(y))})
        return float(res.pvalues[1]), float(res.rsquared)

    try:
        p_P2D_raw, r2_P2D = hac_ols(D_raw, P_raw)
    except Exception:
        p_P2D_raw, r2_P2D = np.nan, np.nan

    try:
        p_D2P_raw, r2_D2P = hac_ols(P_raw, D_raw)
    except Exception:
        p_D2P_raw, r2_D2P = np.nan, np.nan

    # -------- standardized betas (no HAC for speed; these are effect sizes) --------
    try:
        b_P2D = float(sm.OLS(Dz, sm.add_constant(Pz)).fit().params[1])
        b_D2P = float(sm.OLS(Pz, sm.add_constant(Dz)).fit().params[1])
    except Exception:
        b_P2D = b_D2P = np.nan

    # -------- Fast Granger (fixed p) --------
    gP2D = granger_pvalue_fixed(D_raw, P_raw, p=GC_P)  # P -> D
    gD2P = granger_pvalue_fixed(P_raw, D_raw, p=GC_P)  # D -> P

    return {
        "Proximal": prox, "Distal": dist,
        # XCorr (raw)
        "Lag_XCorr_(D_lags_P>0)": lag_DP, "XCorr_peak_abs_(D|P)": pk_DP,
        "Lag_XCorr_(P_lags_D>0)": lag_PD, "XCorr_peak_abs_(P|D)": pk_PD,
        # HAC-OLS (raw)
        "R2_Pâ†’D": r2_P2D, "R2_Dâ†’P": r2_D2P,
        "Pval_Pâ†’D_raw": p_P2D_raw, "Pval_Dâ†’P_raw": p_D2P_raw,
        # standardized betas
        "CoefStd_Pâ†’D": b_P2D, "CoefStd_Dâ†’P": b_D2P,
        # Granger (fixed-lag VAR)
        "Granger_Pâ†’D": gP2D, "Granger_Dâ†’P": gD2P,
        "N_obs": int(n_obs)
    }

# ---------- Main pairwise analysis (parallel) ----------
def analyze_prox_vs_dist(avg_emg: pd.DataFrame) -> pd.DataFrame:
    prox_muscles = ["RF", "VM", "VL"]
    dist_muscles = ["GM", "GL"]
    pairs = [(P, D) for P in prox_muscles for D in dist_muscles]

    out_rows = []
    # IMPORTANT: ensure phase names are normalized before grouping
    df_norm = normalize_phase_names(avg_emg, col=PHASE_COL)
    # drop UnknownPhase for the analysis
    df_norm = df_norm[df_norm[PHASE_COL].isin(PHASE_ORDER)].copy()

    for (group, phase), phase_df in df_norm.groupby([GROUP_COL, PHASE_COL]):
        jobs = (delayed(_analyze_pair)(phase_df, TIME_COL, P, D) for (P, D) in pairs)
        rows = Parallel(n_jobs=N_JOBS, prefer="processes")(jobs)
        for r in rows:
            if r is None:
                continue
            r.update({"Group": group, "Phase": phase})
            out_rows.append(r)

    df = pd.DataFrame(out_rows).round(6)

    # FDR within GroupÃ—Phase, separately for Pâ†’D and Dâ†’P (use Granger p-values)
    if not df.empty:
        df = df.sort_values([GROUP_COL, PHASE_COL, "Proximal", "Distal"]).reset_index(drop=True)
        for colp, colq in [("Granger_Pâ†’D", "q_Pâ†’D"), ("Granger_Dâ†’P", "q_Dâ†’P")]:
            df[colq] = np.nan
            for (g, ph), idx in df.groupby([GROUP_COL, PHASE_COL]).groups.items():
                vec = df.loc[idx, colp].to_numpy(float)
                mask = np.isfinite(vec)
                if mask.sum() > 1:
                    q = np.full_like(vec, np.nan, dtype=float)
                    q[mask] = multipletests(vec[mask], method='fdr_bh')[1]
                    df.loc[idx, colq] = q
    return df

# ---------- Asymmetric phase summary ----------
def summarize_by_phase(pd_df: pd.DataFrame, alpha=0.05, use_fdr=True) -> pd.DataFrame:
    if pd_df is None or pd_df.empty:
        return pd.DataFrame()

    # ensure phase normalized and ordered
    d0 = normalize_phase_names(pd_df, col="Phase")
    d0 = d0[d0["Phase"].isin(PHASE_ORDER)].copy()

    rows = []
    for (g, ph), d in d0.groupby([GROUP_COL, "Phase"]):
        if use_fdr:
            sig_p2d = (d["q_Pâ†’D"] < alpha)
            sig_d2p = (d["q_Dâ†’P"] < alpha)
        else:
            sig_p2d = (d["Granger_Pâ†’D"] < alpha)
            sig_d2p = (d["Granger_Dâ†’P"] < alpha)

        n_pairs     = len(d)
        n_sig_p2d   = int(np.nansum(sig_p2d.astype(float)))
        n_sig_d2p   = int(np.nansum(sig_d2p.astype(float)))
        mean_b_p2d  = np.nanmean(np.abs(d["CoefStd_Pâ†’D"]))
        mean_b_d2p  = np.nanmean(np.abs(d["CoefStd_Dâ†’P"]))
        delta_b     = np.nanmean(np.abs(d["CoefStd_Pâ†’D"]) - np.abs(d["CoefStd_Dâ†’P"]))
        med_xc_dp   = np.nanmedian(d["XCorr_peak_abs_(D|P)"])
        med_xc_pd   = np.nanmedian(d["XCorr_peak_abs_(P|D)"])

        rows.append({
            "Group": g, "Phase": ph,
            "n_pairs": n_pairs,
            "n_sig_Pâ†’D": n_sig_p2d,
            "n_sig_Dâ†’P": n_sig_d2p,
            "DI": (n_sig_p2d - n_sig_d2p) / n_pairs if n_pairs else np.nan,
            "mean|Î²|_Pâ†’D":  mean_b_p2d,
            "mean|Î²|_Dâ†’P":  mean_b_d2p,
            "Î”|Î²| (Pâ†’D âˆ’ Dâ†’P)": delta_b,
            "median |XCorr| (D|P)": med_xc_dp,
            "median |XCorr| (P|D)": med_xc_pd,
        })
    out = pd.DataFrame(rows)
    if out.empty:
        return out
    # enforce canonical phase order in the table
    out["Phase"] = pd.Categorical(out["Phase"], categories=PHASE_ORDER, ordered=True)
    return out.sort_values(["Phase", "Group"]).reset_index(drop=True)

# ---------- One-shot runner ----------
def run_emg2emg_prox_vs_dist(avg_emg: pd.DataFrame,
                             df_raw: pd.DataFrame = None,
                             add_irf: bool = False,
                             fdr_alpha: float = 0.05,
                             fdr_on_fdr: bool = True):
    """
    add_irf=True requires helpers: _select_tvvar_for_pair, tvvar_irf, _irf_summary_stats
    """
    # ensure distal columns exist
    avg2  = ensure_distal_emg(avg_emg, df_raw, require_cols=("GM","GL"))
    # normalize phases early (so pairwise uses canonical names)
    avg2  = normalize_phase_names(avg2, col=PHASE_COL)
    avg2  = avg2[avg2[PHASE_COL].isin(PHASE_ORDER)].copy()

    pd_df = analyze_prox_vs_dist(avg2)

    # Optional IRF (off by default for speed)
    HAVE_IRF = all(name in globals() for name in ["_select_tvvar_for_pair","tvvar_irf","_irf_summary_stats"])
    if add_irf and HAVE_IRF and not pd_df.empty:
        pd_df = add_irf_metrics_emg2emg(
            avg2, pd_df,
            alpha=fdr_alpha, use_fdr=fdr_on_fdr,
            directions='both_if_sig', orth='generalized',
            horizon='auto', stride_ms=DEFAULT_STRIDE_MS
        )

    # normalize result phases (defensive) before summary & visuals
    pd_df = normalize_phase_names(pd_df, col="Phase") if pd_df is not None else pd_df
    if pd_df is not None and not pd_df.empty:
        pd_df = pd_df[pd_df["Phase"].isin(PHASE_ORDER)].copy()

    phase_summary = summarize_by_phase(pd_df, alpha=fdr_alpha, use_fdr=fdr_on_fdr)
    # normalize summary (already ordered inside, but keep consistent dtype)
    phase_summary = normalize_phase_names(phase_summary, col="Phase") if phase_summary is not None else phase_summary
    if phase_summary is not None and not phase_summary.empty:
        phase_summary = phase_summary[phase_summary["Phase"].isin(PHASE_ORDER)].copy()

    return pd_df, phase_summary

# === Run and surface results ===
OUT_DIR = "outputs"
os.makedirs(OUT_DIR, exist_ok=True)

pd_df, phase_summary = run_emg2emg_prox_vs_dist(
    avg_emg=avg_emg,
    df_raw=df_raw,            # or None if not needed
    add_irf=False,            # True only if IRF helpers are defined
    fdr_alpha=0.05,
    fdr_on_fdr=True
)

def _preview(df, name, n=30):
    print(f"\n=== {name} (rows={0 if df is None else len(df)}) ===")
    if df is None or df.empty:
        print("(empty)")
    else:
        with pd.option_context("display.max_rows", n,
                               "display.max_columns", None,
                               "display.width", 160,
                               "display.float_format", "{:.6f}".format):
            print(df.head(n).to_string(index=False))

_preview(pd_df, "EMGâ†”EMG pairwise results", n=30)
_preview(phase_summary, "Phase-wise summary", n=200)

# Save to Excel + CSV
xlsx_path = os.path.join(OUT_DIR, "emg_prox_vs_dist_results.xlsx")
with pd.ExcelWriter(xlsx_path, engine="openpyxl") as xl:
    (pd_df if pd_df is not None else pd.DataFrame()).to_excel(xl, index=False, sheet_name="Pairwise")
    (phase_summary if phase_summary is not None else pd.DataFrame()).to_excel(xl, index=False, sheet_name="PhaseSummary")
print(f"\nğŸ“¦ Saved tables to: {xlsx_path}")

if pd_df is not None and not pd_df.empty:
    pd_df.to_csv(os.path.join(OUT_DIR, "emg_prox_vs_dist_pairwise.csv"), index=False)
if phase_summary is not None and not phase_summary.empty:
    phase_summary.to_csv(os.path.join(OUT_DIR, "emg_prox_vs_dist_phase_summary.csv"), index=False)

# =====================================================================
#  Visualization for EMGâ†”EMG proximal vs distal
#  Creates: DI heatmap + per-phase bar charts (DI, counts, mean |Î²|)
#  Uses canonical 7 named phases in the correct order.
# =====================================================================

def _ordered_groups(phase_summary: pd.DataFrame):
    if phase_summary is None or phase_summary.empty:
        return []
    # If your pipeline defines GROUP_ORDER elsewhere, reuse it; otherwise infer
    go = globals().get("GROUP_ORDER", None)
    if go is not None:
        present = [g for g in go if g in phase_summary["Group"].unique().tolist()]
        if present:
            return present
    # common Italian labels in your printouts
    preferred = ["NORMATIVI", "GRUPPO3", "GRUPPO2", "GRUPPO1",
                 "None", "3", "2", "1"]
    groups = phase_summary["Group"].unique().tolist()
    ordered = [g for g in preferred if g in groups]
    ordered += [g for g in groups if g not in ordered]
    return ordered

def _ordered_phases(phase_summary: pd.DataFrame):
    # Always obey the canonical phase order; show only those present
    present = phase_summary["Phase"].dropna().unique().tolist() if (phase_summary is not None and not phase_summary.empty) else []
    return [p for p in PHASE_ORDER if p in present]

def _ensure_dir(path):
    os.makedirs(path, exist_ok=True)
    return path

def _bar_grouped(ax, xlabels, yA, yB, width=0.38):
    idx = np.arange(len(xlabels))
    ax.bar(idx - width/2, yA, width)
    ax.bar(idx + width/2, yB, width)
    ax.set_xticks(idx)
    ax.set_xticklabels(xlabels, rotation=0)

def _savefig(out_path):
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()
    print(f"ğŸ–¼ï¸  Saved: {out_path}")

def visualize_emg2emg(pd_df: pd.DataFrame, phase_summary: pd.DataFrame, out_dir=OUT_DIR):
    if phase_summary is None or phase_summary.empty:
        print("â„¹ï¸ Visualization skipped: phase_summary is empty.")
        return

    _ensure_dir(out_dir)
    groups = _ordered_groups(phase_summary)
    phases = _ordered_phases(phase_summary)

    # ---------- 1) DI heatmap (Phase Ã— Group) ----------
    pivot_DI = (phase_summary
                .pivot(index="Phase", columns="Group", values="DI")
                .reindex(index=phases, columns=groups))
    H = pivot_DI.fillna(0.0).to_numpy(dtype=float)  # show zeros where missing

    fig, ax = plt.subplots(figsize=(1.6 + 0.9*len(groups), 1.6 + 0.7*len(phases)))
    im = ax.imshow(H, aspect="auto", interpolation="nearest")
    ax.set_xticks(np.arange(len(groups)))
    ax.set_xticklabels(groups, rotation=45, ha="right")
    ax.set_yticks(np.arange(len(phases)))
    ax.set_yticklabels(phases)
    ax.set_title("Directionality Index (DI) â€” Proximal vs Distal\nDI = (n_sig_Pâ†’D âˆ’ n_sig_Dâ†’P) / n_pairs")
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label("DI (âˆ’1â€¦+1)", rotation=90)

    # annotate cells with real DI (keep NaN as Â·)
    for i in range(H.shape[0]):
        for j in range(H.shape[1]):
            val = pivot_DI.iloc[i, j]
            txt = "Â·" if pd.isna(val) else f"{val:.2f}"
            ax.text(j, i, txt, ha="center", va="center", fontsize=9, color="black")
    _savefig(os.path.join(out_dir, "emg2emg_DI_heatmap.png"))

    # ---------- 2) Per-phase bar charts ----------
    for ph in phases:
        d = phase_summary[phase_summary["Phase"] == ph].copy()

        # ensure all groups visible (fill missing with zeros)
        d = d.set_index("Group").reindex(groups)

        # counts
        for col in ["n_sig_Pâ†’D", "n_sig_Dâ†’P", "n_pairs"]:
            if col in d.columns:
                d[col] = d[col].fillna(0).astype(int)

        # DI bars (fill NaN with 0 to show â€œno infoâ€)
        di_vals = d["DI"].fillna(0.0).to_numpy(dtype=float) if "DI" in d.columns else np.zeros(len(d))

        # 2a) DI
        fig, ax = plt.subplots(figsize=(1.2 + 0.7*len(groups), 3.6))
        ax.bar(np.arange(len(groups)), di_vals)
        ax.set_xticks(np.arange(len(groups)))
        ax.set_xticklabels(groups, rotation=0)
        ax.set_ylim(-1.0, 1.0)
        ax.axhline(0, linestyle="--", alpha=0.6)
        ax.set_ylabel("DI")
        ax.set_title(f"DI by Group â€” {ph}")
        _savefig(os.path.join(out_dir, f"emg2emg_DI_phase_{ph}.png"))

        # 2b) significant pairs (Pâ†’D vs Dâ†’P)
        yA = d["n_sig_Pâ†’D"].to_numpy(dtype=float) if "n_sig_Pâ†’D" in d.columns else np.zeros(len(d))
        yB = d["n_sig_Dâ†’P"].to_numpy(dtype=float) if "n_sig_Dâ†’P" in d.columns else np.zeros(len(d))
        fig, ax = plt.subplots(figsize=(1.2 + 0.7*len(groups), 3.6))
        _bar_grouped(ax, groups, yA, yB)
        ax.set_ylabel("# significant pairs (FDR 0.05)")
        ax.set_title(f"Significant pairs â€” {ph}")
        ax.legend(["Pâ†’D", "Dâ†’P"], loc="best")
        _savefig(os.path.join(out_dir, f"emg2emg_sig_counts_phase_{ph}.png"))

        # 2c) mean |Î²| (Pâ†’D vs Dâ†’P)
        yC = d["mean|Î²|_Pâ†’D"].fillna(0.0).to_numpy(dtype=float) if "mean|Î²|_Pâ†’D" in d.columns else np.zeros(len(d))
        yD = d["mean|Î²|_Dâ†’P"].fillna(0.0).to_numpy(dtype=float) if "mean|Î²|_Dâ†’P" in d.columns else np.zeros(len(d))
        fig, ax = plt.subplots(figsize=(1.2 + 0.7*len(groups), 3.6))
        _bar_grouped(ax, groups, yC, yD)
        ax.set_ylabel("Mean |Î²| (standardized)")
        ax.set_title(f"Mean |Î²| â€” {ph}")
        ax.legend(["Pâ†’D", "Dâ†’P"], loc="best")
        _savefig(os.path.join(out_dir, f"emg2emg_mean_abs_beta_phase_{ph}.png"))

# ---- Call the visualizer right after saving tables ----
visualize_emg2emg(pd_df, phase_summary, out_dir=OUT_DIR)


####################################PLOTS with timeseries################################################
# ==================================================================================================================
# 4) Proximal (RF, VM, VL)  vs  Distal (GM, GL) â€” EMGâ†”EMG (XCorr, HAC-OLS, fast Granger + FDR + optional IRF)
# Question 3: Why are proximal muscles more informative than distal ones for predicting pathology/disability, per phase?
# ==================================================================================================================

from matplotlib.cm import get_cmap
from matplotlib.colors import Normalize
from matplotlib.colorbar import ColorbarBase
from matplotlib.collections import PolyCollection
import os
import numpy as np
import matplotlib.pyplot as plt

def visualize_prox_vs_dist_colormapped(group_signals, pd_df, phase_summary, group, out_dir):
    os.makedirs(out_dir, exist_ok=True)

    prox_muscles = ["RF", "VM", "VL"]
    dist_muscles = ["GM", "GL"]
    cmap = get_cmap("seismic")
    norm = Normalize(vmin=-1.0, vmax=1.0, clip=True)

    phases = PHASE_ORDER
    n_phases = len(phases)
    phase_bounds = np.linspace(0, 100, n_phases + 1)

    fig, axes = plt.subplots(nrows=len(prox_muscles), ncols=1, figsize=(14, 3 * len(prox_muscles)), sharex=True)

    for i, prox in enumerate(prox_muscles):
        ax = axes[i]
        prox_signals = group_signals[group][prox][0]
        if not prox_signals or len(prox_signals) == 0:
            ax.set_axis_off()
            continue

        prox_signals = np.array(prox_signals)
        mean_prox = np.mean(prox_signals, axis=0)
        x = np.linspace(0, 100, len(mean_prox))

        # Shade each phase under the proximal EMG curve
        for j, phase in enumerate(phases):
            x0_idx = int(j * len(x) / n_phases)
            x1_idx = int((j + 1) * len(x) / n_phases)
            x_phase = x[x0_idx:x1_idx]
            y_phase = mean_prox[x0_idx:x1_idx]

            if x_phase.size < 2:
                continue

            try:
                di_val = phase_summary.loc[
                    (phase_summary["Group"] == group) &
                    (phase_summary["Phase"] == phase),
                    "DI"
                ].values[0]
            except IndexError:
                di_val = np.nan

            pd_rows = pd_df[
                (pd_df["Group"] == group) &
                (pd_df["Phase"] == phase) &
                (pd_df["Proximal"] == prox)
            ]

            heat_vals = []
            for _, row in pd_rows.iterrows():
                if np.isnan(di_val):
                    continue
                if di_val > 0:
                    heat_vals.append(-abs(row["CoefStd_Pâ†’D"]))  # Blue
                elif di_val < 0:
                    heat_vals.append(abs(row["CoefStd_Dâ†’P"]))   # Red

            if heat_vals:
                avg_val = np.nanmean(heat_vals)
                if np.isfinite(avg_val):
                    shade_color = cmap(norm(avg_val))
                    verts = [(x_phase[0], 0)] + list(zip(x_phase, y_phase)) + [(x_phase[-1], 0)]
                    poly = PolyCollection([verts], facecolor=shade_color, edgecolor='none', alpha=1.0, zorder=0)
                    ax.add_collection(poly)

        # Proximal EMG
        ax.plot(x, mean_prox, color='black', linewidth=1.8)

        # Distal overlays
        dist_styles = {
            "GM": {"linestyle": "--", "color": "dimgray", "linewidth": 1.2},
            "GL": {"linestyle": "-.", "color": "dimgray", "linewidth": 1.2},
        }

        for dist in dist_muscles:
            dist_signals = group_signals[group][dist][0]
            if dist_signals and len(dist_signals) > 0:
                dist_signals = np.array(dist_signals)
                mean_dist = np.mean(dist_signals, axis=0)
                ax.plot(x, mean_dist, **dist_styles[dist])

        ax.set_title(f"{prox} - {group}", fontsize=11)
        ax.set_ylim([0, 1])
        ax.set_ylabel("EMG (norm.)")
        ax.set_xticks(np.linspace(0, 100, 5))
        ax.set_xlim([0, 100])
        ax.grid(False)

        if i == len(prox_muscles) - 1:
            ax.set_xlabel("Gait Cycle (%)")

    # Colorbar
    cax = fig.add_axes([0.92, 0.15, 0.015, 0.7])
    cb = ColorbarBase(cax, cmap=cmap, norm=norm, orientation='vertical')
    cb.set_label("EMGâ†’EMG Directionality\nâ† Proxâ†’Dist (blue), Distâ†’Prox â†’ (red)\nIntensity = CoefStd", fontsize=10)

    fig.suptitle(f"EMG Timeseries â€” {group}", fontsize=16, y=0.98)
    fig.tight_layout(rect=[0.03, 0.04, 0.90, 0.95])

    out_path = os.path.join(out_dir, f"emg2emg_colormapped_{group}.png")
    plt.savefig(out_path, dpi=200)
    plt.close(fig)
    print(f"ğŸ–¼ï¸  Saved group figure with seismic colormap: {out_path}")

for group in group_signals.keys():
    if group not in pd_df["Group"].unique():
        print(f"âš ï¸ Skipping {group}: no results found in pd_df.")
        continue
    visualize_prox_vs_dist_colormapped(
        group_signals=group_signals,
        pd_df=pd_df,
        phase_summary=phase_summary,
        group=group,
        out_dir=OUT_DIR
    )

